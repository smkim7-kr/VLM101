{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization tutorial\n",
    "(Reference : https://www.youtube.com/watch?v=zduSFxRajkE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54620, 44397, 50612, 32, 51339, 50500, 54644, 50836, 32, 40, 10355, 10357, 10359, 41, 32, 65295, 20154, 9685, 8255, 8255, 9685, 20154, 65340]\n"
     ]
    }
   ],
   "source": [
    "# ord() converts character to unicode code points\n",
    "# Unicode is updated frequently, about 140000 characters supported\n",
    "# Cannot use this nativaly since too many vocabs & and update frequently, thus not stable\n",
    "str = \"í•œêµ­ì–´ ì¢‹ì•„í•´ìš” (â¡³â¡µâ¡·) ï¼äººâ—•â€¿â€¿â—•äººï¼¼\"\n",
    "print([ord(x) for x in str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  í•œêµ­ì–´ ì¢‹ì•„í•´ìš” (â¡³â¡µâ¡·) ï¼äººâ—•â€¿â€¿â—•äººï¼¼\n",
      "String length:  23\n",
      "Encoded:  [237, 149, 156, 234, 181, 173, 236, 150, 180, 32, 236, 162, 139, 236, 149, 132, 237, 149, 180, 236, 154, 148, 32, 40, 226, 161, 179, 226, 161, 181, 226, 161, 183, 41, 32, 239, 188, 143, 228, 186, 186, 226, 151, 149, 226, 128, 191, 226, 128, 191, 226, 151, 149, 228, 186, 186, 239, 188, 188]\n",
      "Encoded length:  59\n"
     ]
    }
   ],
   "source": [
    "# Unicode can encoded using into binary data\n",
    "# Types: utf-8 (mostly used), utf-16, utf-32\n",
    "print(\"String: \", str)\n",
    "print(\"String length: \", len(str))\n",
    "tokens = list(str.encode(\"utf-8\"))\n",
    "print(\"Encoded: \", tokens)\n",
    "print(\"Encoded length: \", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded (utf-32):  [255, 254, 0, 0, 92, 213, 0, 0, 109, 173, 0, 0, 180, 197, 0, 0, 32, 0, 0, 0, 139, 200, 0, 0, 68, 197, 0, 0, 116, 213, 0, 0, 148, 198, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 115, 40, 0, 0, 117, 40, 0, 0, 119, 40, 0, 0, 41, 0, 0, 0, 32, 0, 0, 0, 15, 255, 0, 0, 186, 78, 0, 0, 213, 37, 0, 0, 63, 32, 0, 0, 63, 32, 0, 0, 213, 37, 0, 0, 186, 78, 0, 0, 60, 255, 0, 0]\n",
      "Encoded length (utf-32):  96\n"
     ]
    }
   ],
   "source": [
    "# Other utf types are not efficient in many cases\n",
    "# Here utf-32 includes several \"0\" token, thus longer token length compare to utf-8 encoding\n",
    "print(\"Encoded (utf-32): \", list(str.encode(\"utf-32\")))\n",
    "print(\"Encoded length (utf-32): \", len(list(str.encode(\"utf-32\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte Pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte pair encoding replaces frequent byte pair into a single byte\n",
    "# e.g. xyabxyxy => zabzz where z = xy\n",
    "# This reduces the length of token sequences\n",
    "# Transformers with fixed context length, it can effectively attend larger context (as multiple tokens' context are squeezed into single token)\n",
    "str = \"\"\"\n",
    "í˜ì´ì»¤ëŠ” ê²Œì„ ë‚´ë¶€ì ìœ¼ë¡œ í˜„ëŒ€ ë¯¸ë“œë¼ì´ë„ˆì˜ ê°œë…ì„ ì •ë¦½í•œ ì„ ìˆ˜ë¡œ í‰ê°€ëœë‹¤. í˜ì´ì»¤ì˜ ë“±ì¥ì€ ê³ ì „ì  1ì„¸ëŒ€ ë¯¸ë“œë¼ì´ë„ˆ ì‹œëŒ€ì˜ ì¢…ì–¸ì„ ì•Œë ¸ë‹¤. ì‹œì¦Œ 2ê¹Œì§€ ë¯¸ë“œ ë¼ì´ë„ˆëŠ” ë¬´ë¦¬í•˜ì§€ ì•Šê³  íŒŒë°ì„ í•˜ë©´ì„œ ì„±ì¥í•˜ê³  ì£¼ë¡œ í•„ìš”í•œ ìˆœê°„ì—ë§Œ í•©ë¥˜ë¥¼ í–ˆëŠ”ë°, 2013 ì‹œì¦Œë¶€í„° ë“±ì¥í•œ í˜ì´ì»¤ëŠ” ìƒëŒ€ ë¼ì´ë„ˆë¥¼ ëŠì„ì—†ì´ ì••ë°•í•˜ë©´ì„œ ì´ë“ì„ ì·¨í•˜ëŠ” í”Œë ˆì´ë¥¼ ì‹œì‘í–ˆë‹¤.\n",
    "ìœ ëª… ì•„ë§ˆì¶”ì–´ ê²Œì´ë¨¸ì¸ ë„íŒŒëŠ” \"ë¯¸ë“œ ë¼ì´ë„ˆë¡œì„œ ìµœê³ ì˜ í”Œë ˆì´ëŠ” ìƒëŒ€ë°©ì„ ê·¹ë‹¨ì ìœ¼ë¡œ ì••ë°•í•´ ì  ì •ê¸€ì„ ë¶ˆëŸ¬ë“¤ì¼ ìˆ˜ë°–ì— ì—†ëŠ” ìƒí™©ì„ ë§Œë“¤ê³  2:1 ìƒí™©ì„ ë§Œë“  í›„ì— ì£½ì§€ ì•ŠëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ 2:1 ë“œë˜ë¸”ë§ì„ ìµœì´ˆë¡œ ì‹œì‘í•œ ê²ƒì´ í˜ì´ì»¤ì˜€ê³  ì´ë¥¼ ìµœì í™”ì‹œì¼œ ì™„ì„±í•œ ì‚¬ëŒì´ ë£¨í‚¤ì˜€ë‹¤.\"ë¼ê³  í‰ê°€í–ˆë‹¤.[101]\n",
    "ì¦‰ ìƒëŒ€í•˜ëŠ” ì…ì¥ì—ì„œ ì  ì •ê¸€ì„ ë¶ˆëŸ¬ë“¤ì¼ ìˆ˜ë°–ì— ì—†ëŠ” ìƒí™©ì„ ë§Œë“  ë’¤ì— ì  ì •ê¸€ì´ ê°±ì´ë‚˜ ê²¬ì œë¥¼ í•´ì˜¤ë©´ ë³¸ì¸ì€ ì–´ê·¸ë¡œë¥¼ ëŒë©´ì„œ íŒ€ì ìœ¼ë¡œ ìƒëŒ€ ì •ê¸€ì˜ ìœ„ì¹˜ë¥¼ ì´ìš©í•´ ë‹¤ë¥¸ ë¼ì¸ì— í˜ì„ ì‹¤ì–´ì£¼ê¸° í˜¹ì€ ì—­ê°±ì„ ì„±ê³µì‹œì¼œ ê·¸ì— ë”°ë¥¸ ìŠ¤ë…¸ìš°ë³¼ì„ êµ´ë¦¬ê±°ë‚˜ ìµœì ì˜ ìƒí™©ìœ¼ë¡œëŠ” 1ëŒ€2 ì—­ê´€ê´‘ì„ í•´ ê²Œì„ì„ í„°ëœ¨ë¦¬ëŠ” ê²ƒì´ë‹¤. ë°˜ëŒ€ë¡œ ë³¸ì¸ì—ê²Œ ê²¬ì œê°€ ì˜¤ì§€ ì•ŠëŠ”ë‹¤ë©´, ë¼ì¸ ì£¼ë„ê¶Œì„ ì´ìš©í•´ í•œë°œ ë¹ ë¥¸ í•©ë¥˜ì™€ íƒ€ ë¼ì¸ì— ëŒ€í•œ ê²¬ì œë¥¼ ë„£ê±°ë‚˜ ì•„ì˜ˆ ë¯¸ë“œ ë¼ì¸ì—ì„œ ì†”í‚¬ì„ ë‚´ë²„ë¦¬ê³  í­íŒŒì‹œì¼œë²„ë¦¬ëŠ” ë“±, ë‹¤ì‹œ ë§í•´ ì  ë¯¸ë“œ ì •ê¸€ì„ ìê¸° ë§ˆìŒëŒ€ë¡œ ì»¨íŠ¸ë¡¤í•˜ì—¬ ê²Œì„ì„ í„°ëœ¨ë¦¬ëŠ”, ì†Œìœ„ ìŠ¤í¬ì¸ ì—ì„œ í†µì¹­ë˜ëŠ” 'í¬ë™ í”Œë ˆì´'ì˜ ì •ì ì— ê°€ê¹Œì› ë˜ ì„ ìˆ˜ì˜€ë‹¤.\n",
    "ì‹¤ì œë¡œ ì´ 2ëŒ€1 ë“œë¦¬ë¸”ë§ì´ ê°€ëŠ¥í•œ ì§€ ì—¬ë¶€ëŠ” ì´í›„ ë¯¸ë“œ, íƒ‘ ì„ ìˆ˜ ì¤‘ Sê¸‰ê³¼ Aê¸‰ì„ ê°€ë¥´ëŠ” ì¤‘ìš”í•œ ì§€í‘œì¤‘ í•˜ë‚˜ê°€ ë˜ì—ˆë‹¤. í˜ì´ì»¤ê°€ ìµœì´ˆë¡œ ì‹œì‘í•˜ê³  ë£¨í‚¤ê°€ ìµœì í™”ì‹œí‚¨ í›„ 2015 ì‹œì¦Œë¶€í„° ë³¸ê²©ì ìœ¼ë¡œ ë§ˆë¦°, ìŠ¤ë§µ, íë² , ì¹¸, ë”ìƒ¤ì´, ë¹„ë””ë””, ìµ¸ë¹„ ë“± ì†ì¹­ \"Sê¸‰ ì„ ìˆ˜\"ë“¤ì´ ëŒ€ê±° ë“±ì¥í•˜ê¸° ì‹œì‘í–ˆë‹¤. [102]\n",
    "ì‹¤ì œë¡œ ì´ëŸ¬í•œ í”Œë ˆì´ì˜ ê³¼ì •ì—ì„œ ë‚˜ì˜¤ëŠ” í˜ì´ì»¤ íŠ¹ìœ ì˜ ì™¸ì¤„íƒ€ê¸°, ì†”ë¡œ í‚¬, ë„“ì€ ì±”í”„í­, ìŠˆí¼ í”Œë ˆì´ëŠ” ì‚¬ëŒë“¤ì„ ì—´ê´‘í•˜ê²Œ í•´ ê²½ê¸° ì™¸ì ìœ¼ë¡œ ë§‰ ì¸ê¸°ë¥¼ ì–»ê¸° ì‹œì‘í•˜ë˜ ë¦¬ê·¸ ì˜¤ë¸Œ ë ˆì „ë“œì™€ ë¡¤ì±”ìŠ¤ì˜ ìœ ëª…ì„¸ë¥¼ ë”ìš± ì¦í­ì‹œí‚¤ëŠ” ê¸°í­ì œ ì—­í• ì„ í•˜ê¸°ë„ í–ˆê³  ë¥´ë¸”ë‘, ì•„ë¦¬, ì œë“œ ë“±ì˜ ì•”ì‚´ì ì±”í”¼ì–¸ì˜ ì¸ê¸°ê°€ ê¸‰ìƒìŠ¹í•˜ê¸°ë„ í–ˆë‹¤.\n",
    "ì´ íŒ¨ëŸ¬ë‹¤ì„ì€ 2018 ì‹œì¦Œê¹Œì§€ ë¯¸ë“œë¼ì´ë„ˆì˜ ìœ ì¼í•œ í¬ë™ í”Œë ˆì´ë¡œ ë‚¨ì•„ìˆì—ˆë‹¤. í•˜ì§€ë§Œ 2019 ì‹œì¦Œ íƒì›”í•œ ìš´ì˜ê³¼ ë¡œë°ìœ¼ë¡œ ìŠ¹ë¶€ë¥¼ ë³´ëŠ” ë„ì¸ë¹„ì™€ ìº¡ìŠ¤ì˜ ë“±ì¥ê³¼ 2020 ì‹œì¦Œ ìœ¡ê°í˜• í”Œë ˆì´ë¡œ LCK ì•”í‘ê¸°ë¥¼ ëë‚¸ ì‡¼ë©”ì´ì»¤ ì´í›„ë¡œ ë” ì´ìƒ ìƒëŒ€ ë¯¸ë“œë¥¼ ê·¹í•œìœ¼ë¡œ ì••ë°•í•˜ì§€ ì•Šê³  ë§µì„ ë„“ê²Œ ì“°ëŠ” ë°©ì‹ìœ¼ë¡œë„ ìºë¦¬í•  ìˆ˜ ìˆë‹¤ëŠ” í•´ë²•ì´ ë‚˜ì˜¤ë©´ì„œ ì „ì²´ì ì¸ ë¯¸ë“œ í”Œë ˆì´ ìŠ¤íƒ€ì¼ì€ í¬ê²Œ ì´ ë‘˜ë¡œ ë‚˜ë‰˜ê²Œ ë˜ì—ˆë‹¤. ê·¸ë¦¬ê³  ë¯¸ë“œ ë¼ì´ë„ˆë“¤ì˜ ìƒí–¥í‰ì¤€í™”ì™€ 2022 ì‹œì¦Œ ë‚´êµ¬ë„ íŒ¨ì¹˜ë¡œ ì¸í•´ ë” ì´ìƒ í”„ë¡œ ë ˆë²¨ì—ì„œ ìƒëŒ€ë°©ì„ ì••ë„í•˜ê¸°ê°€ í˜ë“¤ì–´ì§€ì ë„ì¸ë¹„ì™€ ìº¡ìŠ¤, ì‡¼ë©”ì´ì»¤ ìŠ¤íƒ€ì¼ì´ ìœ í–‰í•˜ê¸° ì‹œì‘í–ˆë‹¤.[103][104]\n",
    "ê·¸ ì™¸ì—ë„ ë‹¨ìˆœíˆ ë‚´ ëˆˆì•ì— ì„œ ìˆëŠ” ìƒëŒ€ë°©ì„ ë„ë¥™í•˜ëŠ” ê²ƒë§Œì´ ì•„ë‹ˆë¼ ì™€ë“œë¥¼ ë°•ëŠ” ìœ„ì¹˜ë‚˜ íƒ€ì´ë°, ì´ë¥¼ ì´ìš©í•œ ìƒëŒ€ë°© ë¯¸ë“œì™€ ì •ê¸€ ìœ„ì¹˜ ì°¾ê¸° ë“±ì˜ ì§€ëŠ¥ì  í”Œë ˆì´ ì—­ì‹œ ì†”ë¡œë­í¬ ë¦¬í”Œë ˆì´ì™€ ê´€ì „ ë“±ìœ¼ë¡œ ì•Œë ¤ì§„ ê²ƒë§Œ ì—¬ëŸ¿ ëœë‹¤.[105][106] ì¢…í•©ì ìœ¼ë¡œ í˜ì´ì»¤ëŠ” ê²Œì„ ë‚´ì™¸ì— ì—„ì²­ë‚œ ì˜í–¥ì„ ë¼ì¹œ ì„ ìˆ˜ë¡œ, ë‹¨ê¸°ì ìœ¼ë¡œë‚˜ë§ˆ í˜ì´ì»¤ ê¸‰ì˜ ì˜í–¥ë ¥ì„ ë¼ì¹œ ì„ ìˆ˜ëŠ” ì„œí¬í„°ë¥¼ ë„˜ì–´ ì•„ì˜ˆ ë¡¤íŒì˜ ê¸°ë³¸ ìš´ì˜ì„ ì •ë¦½í•œ ë§ˆíƒ€, LPL íƒ‘ ë¼ì¸ì— ê¸‰ë³€ì„ ë¶ˆëŸ¬ì™€ ìŠˆí¼ìŠ¤íƒ€ì— ë“±ê·¹í•œ ë”ìƒ¤ì´, ì»¤ë¦¬ì–´ëŠ” ì•ì„  ì„ ìˆ˜ë“¤ì— ë¹„í•´ ë¶€ì¡±í•˜ì§€ë§Œ íŒ€ì˜ 1ì˜µì…˜ìœ¼ë¡œ í¬ì§€ì…˜ì˜ ì¸ì‹ì„ ë’¤ë°”ê¾¼ ë¡¤íŒì˜ ì„ìš”í™˜ ë§¤ë“œë¼ì´í”„ì •ë„ ë¿ì´ë‹¤. ì¦‰, ë¦¬ê·¸ì™€ ê²Œì„ì„ ë°”ê¾¼ ê°œì¸.\n",
    "\"\"\"\n",
    "tokens = list(str.encode(\"utf-8\"))\n",
    "tokens = list(map(int, tokens)) # for convinience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  4112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10, 237, 142, 152, 236, 157, 180, 236, 187, 164]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Length: \", len(tokens))\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(10, 237): 1,\n",
       " (237, 142): 8,\n",
       " (142, 152): 8,\n",
       " (152, 236): 23,\n",
       " (236, 157): 141,\n",
       " (157, 180): 60,\n",
       " (180, 236): 29,\n",
       " (236, 187): 12,\n",
       " (187, 164): 11,\n",
       " (164, 235): 16,\n",
       " (235, 138): 33,\n",
       " (138, 148): 31,\n",
       " (148, 32): 30,\n",
       " (32, 234): 35,\n",
       " (234, 178): 21,\n",
       " (178, 140): 11,\n",
       " (140, 236): 14,\n",
       " (236, 158): 27,\n",
       " (158, 132): 8,\n",
       " (132, 32): 46,\n",
       " (32, 235): 112,\n",
       " (235, 130): 17,\n",
       " (130, 180): 6,\n",
       " (180, 235): 31,\n",
       " (235, 182): 9,\n",
       " (182, 128): 6,\n",
       " (128, 236): 18,\n",
       " (236, 160): 40,\n",
       " (160, 129): 17,\n",
       " (129, 236): 14,\n",
       " (236, 156): 23,\n",
       " (156, 188): 13,\n",
       " (188, 235): 17,\n",
       " (235, 161): 37,\n",
       " (161, 156): 33,\n",
       " (156, 32): 66,\n",
       " (32, 237): 60,\n",
       " (237, 152): 3,\n",
       " (152, 132): 1,\n",
       " (132, 235): 11,\n",
       " (235, 140): 17,\n",
       " (140, 128): 19,\n",
       " (128, 32): 35,\n",
       " (235, 175): 12,\n",
       " (175, 184): 12,\n",
       " (184, 235): 22,\n",
       " (235, 147): 39,\n",
       " (147, 156): 18,\n",
       " (156, 235): 25,\n",
       " (235, 157): 15,\n",
       " (157, 188): 20,\n",
       " (188, 236): 21,\n",
       " (235, 132): 11,\n",
       " (132, 136): 7,\n",
       " (136, 236): 17,\n",
       " (157, 152): 21,\n",
       " (152, 32): 31,\n",
       " (234, 176): 17,\n",
       " (176, 156): 3,\n",
       " (235, 133): 2,\n",
       " (133, 144): 1,\n",
       " (144, 236): 10,\n",
       " (157, 132): 32,\n",
       " (32, 236): 176,\n",
       " (160, 149): 11,\n",
       " (149, 235): 8,\n",
       " (235, 166): 18,\n",
       " (166, 189): 2,\n",
       " (189, 237): 2,\n",
       " (237, 149): 52,\n",
       " (149, 156): 16,\n",
       " (236, 132): 25,\n",
       " (132, 160): 8,\n",
       " (160, 236): 13,\n",
       " (236, 136): 12,\n",
       " (136, 152): 10,\n",
       " (152, 235): 24,\n",
       " (237, 143): 10,\n",
       " (143, 137): 3,\n",
       " (137, 234): 3,\n",
       " (176, 128): 11,\n",
       " (128, 235): 17,\n",
       " (235, 144): 5,\n",
       " (144, 156): 2,\n",
       " (235, 139): 25,\n",
       " (139, 164): 24,\n",
       " (164, 46): 16,\n",
       " (46, 32): 9,\n",
       " (164, 236): 23,\n",
       " (147, 177): 10,\n",
       " (177, 236): 10,\n",
       " (158, 165): 6,\n",
       " (165, 236): 5,\n",
       " (157, 128): 6,\n",
       " (234, 179): 18,\n",
       " (179, 160): 13,\n",
       " (160, 132): 4,\n",
       " (132, 236): 24,\n",
       " (129, 32): 8,\n",
       " (32, 49): 3,\n",
       " (49, 236): 2,\n",
       " (132, 184): 2,\n",
       " (136, 32): 5,\n",
       " (236, 139): 26,\n",
       " (139, 156): 21,\n",
       " (236, 162): 2,\n",
       " (162, 133): 2,\n",
       " (133, 236): 3,\n",
       " (236, 150): 9,\n",
       " (150, 184): 2,\n",
       " (184, 236): 19,\n",
       " (236, 149): 20,\n",
       " (149, 140): 2,\n",
       " (140, 235): 22,\n",
       " (235, 160): 15,\n",
       " (160, 184): 1,\n",
       " (156, 236): 27,\n",
       " (236, 166): 10,\n",
       " (166, 140): 7,\n",
       " (140, 32): 14,\n",
       " (32, 50): 10,\n",
       " (50, 234): 1,\n",
       " (234, 185): 3,\n",
       " (185, 140): 3,\n",
       " (236, 167): 14,\n",
       " (167, 128): 13,\n",
       " (136, 235): 23,\n",
       " (235, 172): 1,\n",
       " (172, 180): 1,\n",
       " (166, 172): 15,\n",
       " (172, 237): 5,\n",
       " (149, 152): 20,\n",
       " (149, 138): 4,\n",
       " (138, 234): 2,\n",
       " (160, 32): 15,\n",
       " (237, 140): 9,\n",
       " (140, 140): 3,\n",
       " (235, 176): 18,\n",
       " (176, 141): 3,\n",
       " (141, 236): 3,\n",
       " (235, 169): 8,\n",
       " (169, 180): 6,\n",
       " (132, 156): 12,\n",
       " (132, 177): 3,\n",
       " (165, 237): 5,\n",
       " (152, 234): 10,\n",
       " (236, 163): 4,\n",
       " (163, 188): 3,\n",
       " (149, 132): 7,\n",
       " (236, 154): 10,\n",
       " (154, 148): 3,\n",
       " (148, 237): 8,\n",
       " (136, 156): 2,\n",
       " (156, 234): 3,\n",
       " (176, 132): 1,\n",
       " (236, 151): 36,\n",
       " (151, 144): 21,\n",
       " (144, 235): 4,\n",
       " (235, 167): 20,\n",
       " (167, 140): 8,\n",
       " (149, 169): 3,\n",
       " (169, 235): 2,\n",
       " (235, 165): 24,\n",
       " (165, 152): 2,\n",
       " (165, 188): 16,\n",
       " (188, 32): 25,\n",
       " (237, 150): 11,\n",
       " (150, 136): 7,\n",
       " (148, 235): 8,\n",
       " (235, 141): 8,\n",
       " (141, 176): 1,\n",
       " (176, 44): 3,\n",
       " (44, 32): 22,\n",
       " (50, 48): 7,\n",
       " (48, 49): 5,\n",
       " (49, 51): 1,\n",
       " (51, 32): 1,\n",
       " (128, 237): 7,\n",
       " (237, 132): 5,\n",
       " (132, 176): 5,\n",
       " (176, 32): 10,\n",
       " (236, 131): 18,\n",
       " (131, 129): 17,\n",
       " (129, 235): 8,\n",
       " (235, 129): 5,\n",
       " (129, 138): 1,\n",
       " (138, 236): 1,\n",
       " (151, 134): 3,\n",
       " (134, 236): 1,\n",
       " (180, 32): 27,\n",
       " (149, 149): 4,\n",
       " (176, 149): 4,\n",
       " (149, 237): 3,\n",
       " (147, 157): 1,\n",
       " (157, 236): 3,\n",
       " (236, 183): 1,\n",
       " (183, 168): 1,\n",
       " (168, 237): 4,\n",
       " (237, 148): 14,\n",
       " (148, 140): 10,\n",
       " (160, 136): 12,\n",
       " (158, 145): 7,\n",
       " (145, 237): 7,\n",
       " (46, 10): 4,\n",
       " (10, 236): 5,\n",
       " (156, 160): 5,\n",
       " (160, 235): 4,\n",
       " (235, 170): 2,\n",
       " (170, 133): 2,\n",
       " (133, 32): 1,\n",
       " (167, 136): 5,\n",
       " (236, 182): 1,\n",
       " (182, 148): 1,\n",
       " (148, 236): 11,\n",
       " (150, 180): 6,\n",
       " (235, 168): 1,\n",
       " (168, 184): 1,\n",
       " (157, 184): 16,\n",
       " (184, 32): 10,\n",
       " (235, 143): 12,\n",
       " (143, 132): 12,\n",
       " (132, 237): 5,\n",
       " (32, 34): 2,\n",
       " (34, 235): 3,\n",
       " (236, 181): 7,\n",
       " (181, 156): 6,\n",
       " (176, 169): 5,\n",
       " (169, 236): 10,\n",
       " (234, 183): 10,\n",
       " (183, 185): 3,\n",
       " (185, 235): 2,\n",
       " (139, 168): 3,\n",
       " (168, 236): 5,\n",
       " (149, 180): 10,\n",
       " (149, 234): 6,\n",
       " (234, 184): 29,\n",
       " (184, 128): 6,\n",
       " (182, 136): 3,\n",
       " (235, 159): 6,\n",
       " (159, 172): 5,\n",
       " (172, 235): 14,\n",
       " (147, 164): 8,\n",
       " (176, 150): 2,\n",
       " (150, 236): 2,\n",
       " (144, 32): 15,\n",
       " (134, 235): 2,\n",
       " (129, 237): 10,\n",
       " (237, 153): 8,\n",
       " (153, 169): 4,\n",
       " (164, 234): 3,\n",
       " (50, 58): 2,\n",
       " (58, 49): 2,\n",
       " (49, 32): 3,\n",
       " (147, 160): 2,\n",
       " (237, 155): 4,\n",
       " (155, 132): 4,\n",
       " (163, 189): 1,\n",
       " (189, 236): 1,\n",
       " (138, 235): 2,\n",
       " (178, 131): 5,\n",
       " (131, 236): 3,\n",
       " (183, 184): 7,\n",
       " (172, 234): 7,\n",
       " (235, 158): 7,\n",
       " (158, 152): 3,\n",
       " (235, 184): 4,\n",
       " (184, 148): 3,\n",
       " (167, 129): 2,\n",
       " (236, 180): 2,\n",
       " (180, 136): 2,\n",
       " (236, 152): 15,\n",
       " (152, 128): 3,\n",
       " (128, 234): 5,\n",
       " (153, 148): 3,\n",
       " (236, 188): 3,\n",
       " (188, 156): 3,\n",
       " (236, 153): 15,\n",
       " (153, 132): 1,\n",
       " (177, 237): 2,\n",
       " (236, 130): 3,\n",
       " (130, 172): 4,\n",
       " (158, 140): 2,\n",
       " (235, 163): 2,\n",
       " (163, 168): 2,\n",
       " (237, 130): 6,\n",
       " (130, 164): 3,\n",
       " (46, 34): 1,\n",
       " (188, 234): 2,\n",
       " (46, 91): 3,\n",
       " (91, 49): 6,\n",
       " (49, 48): 6,\n",
       " (49, 93): 1,\n",
       " (93, 10): 3,\n",
       " (166, 137): 2,\n",
       " (137, 32): 3,\n",
       " (158, 133): 1,\n",
       " (235, 146): 2,\n",
       " (146, 164): 2,\n",
       " (176, 177): 2,\n",
       " (130, 152): 9,\n",
       " (178, 172): 3,\n",
       " (172, 236): 8,\n",
       " (160, 156): 7,\n",
       " (152, 164): 5,\n",
       " (235, 179): 7,\n",
       " (179, 184): 4,\n",
       " (180, 234): 3,\n",
       " (129, 140): 1,\n",
       " (156, 132): 4,\n",
       " (236, 185): 9,\n",
       " (185, 152): 4,\n",
       " (154, 169): 3,\n",
       " (169, 237): 3,\n",
       " (165, 184): 3,\n",
       " (237, 158): 3,\n",
       " (184, 176): 17,\n",
       " (152, 185): 1,\n",
       " (185, 236): 2,\n",
       " (151, 173): 4,\n",
       " (173, 234): 2,\n",
       " (177, 234): 2,\n",
       " (179, 181): 1,\n",
       " (181, 236): 4,\n",
       " (235, 148): 3,\n",
       " (148, 176): 1,\n",
       " (176, 235): 13,\n",
       " (236, 138): 13,\n",
       " (138, 164): 9,\n",
       " (133, 184): 1,\n",
       " (154, 176): 1,\n",
       " (179, 188): 5,\n",
       " (234, 181): 2,\n",
       " (181, 180): 1,\n",
       " (234, 177): 3,\n",
       " (177, 176): 3,\n",
       " (49, 235): 1,\n",
       " (128, 50): 1,\n",
       " (50, 32): 2,\n",
       " (234, 180): 4,\n",
       " (180, 128): 2,\n",
       " (180, 145): 2,\n",
       " (145, 236): 2,\n",
       " (235, 156): 2,\n",
       " (156, 168): 2,\n",
       " (168, 235): 3,\n",
       " (176, 152): 1,\n",
       " (144, 234): 2,\n",
       " (180, 44): 3,\n",
       " (132, 234): 1,\n",
       " (234, 182): 1,\n",
       " (182, 140): 1,\n",
       " (235, 185): 6,\n",
       " (185, 160): 1,\n",
       " (153, 128): 10,\n",
       " (237, 131): 10,\n",
       " (131, 128): 7,\n",
       " (132, 163): 1,\n",
       " (163, 234): 1,\n",
       " (152, 136): 2,\n",
       " (236, 134): 5,\n",
       " (134, 148): 3,\n",
       " (235, 178): 5,\n",
       " (178, 132): 2,\n",
       " (143, 173): 4,\n",
       " (173, 237): 3,\n",
       " (177, 44): 1,\n",
       " (167, 144): 1,\n",
       " (144, 237): 1,\n",
       " (158, 144): 3,\n",
       " (157, 140): 1,\n",
       " (187, 168): 1,\n",
       " (237, 138): 2,\n",
       " (138, 184): 1,\n",
       " (161, 164): 4,\n",
       " (164, 237): 7,\n",
       " (151, 172): 3,\n",
       " (172, 32): 2,\n",
       " (148, 44): 2,\n",
       " (134, 140): 1,\n",
       " (143, 172): 3,\n",
       " (236, 184): 1,\n",
       " (184, 160): 1,\n",
       " (237, 134): 1,\n",
       " (134, 181): 1,\n",
       " (185, 173): 2,\n",
       " (173, 235): 2,\n",
       " (144, 152): 3,\n",
       " (32, 39): 1,\n",
       " (39, 237): 1,\n",
       " (237, 129): 5,\n",
       " (129, 172): 4,\n",
       " (158, 153): 2,\n",
       " (153, 32): 2,\n",
       " (180, 39): 1,\n",
       " (39, 236): 1,\n",
       " (149, 236): 3,\n",
       " (160, 144): 1,\n",
       " (236, 155): 2,\n",
       " (155, 160): 1,\n",
       " (141, 152): 2,\n",
       " (50, 235): 1,\n",
       " (128, 49): 1,\n",
       " (138, 165): 2,\n",
       " (180, 237): 3,\n",
       " (156, 44): 2,\n",
       " (131, 145): 2,\n",
       " (145, 32): 4,\n",
       " (236, 164): 5,\n",
       " (164, 145): 3,\n",
       " (32, 83): 1,\n",
       " (83, 234): 2,\n",
       " (184, 137): 6,\n",
       " (32, 65): 1,\n",
       " (65, 234): 1,\n",
       " (137, 236): 4,\n",
       " (165, 180): 2,\n",
       " (237, 145): 1,\n",
       " (145, 156): 1,\n",
       " (151, 136): 3,\n",
       " (156, 237): 4,\n",
       " (130, 168): 2,\n",
       " (168, 32): 1,\n",
       " (49, 53): 1,\n",
       " (53, 32): 1,\n",
       " (184, 234): 3,\n",
       " (178, 169): 1,\n",
       " (166, 176): 1,\n",
       " (167, 181): 2,\n",
       " (181, 44): 1,\n",
       " (129, 144): 1,\n",
       " (178, 160): 1,\n",
       " (160, 44): 1,\n",
       " (185, 184): 1,\n",
       " (184, 44): 1,\n",
       " (141, 148): 5,\n",
       " (131, 164): 2,\n",
       " (185, 132): 5,\n",
       " (148, 148): 2,\n",
       " (181, 184): 1,\n",
       " (177, 32): 2,\n",
       " (134, 141): 1,\n",
       " (173, 32): 1,\n",
       " (34, 83): 1,\n",
       " (152, 34): 1,\n",
       " (32, 91): 1,\n",
       " (48, 50): 3,\n",
       " (50, 93): 1,\n",
       " (164, 32): 4,\n",
       " (138, 185): 3,\n",
       " (153, 184): 4,\n",
       " (164, 132): 1,\n",
       " (172, 44): 2,\n",
       " (132, 147): 2,\n",
       " (147, 236): 1,\n",
       " (236, 177): 3,\n",
       " (177, 148): 3,\n",
       " (148, 132): 3,\n",
       " (173, 44): 1,\n",
       " (138, 136): 2,\n",
       " (136, 237): 3,\n",
       " (237, 141): 2,\n",
       " (141, 188): 2,\n",
       " (151, 180): 1,\n",
       " (178, 189): 1,\n",
       " (189, 234): 1,\n",
       " (167, 137): 1,\n",
       " (150, 187): 1,\n",
       " (187, 234): 1,\n",
       " (184, 140): 1,\n",
       " (154, 177): 1,\n",
       " (166, 157): 1,\n",
       " (157, 237): 1,\n",
       " (173, 236): 3,\n",
       " (176, 237): 1,\n",
       " (149, 160): 2,\n",
       " (136, 234): 1,\n",
       " (145, 44): 1,\n",
       " (149, 148): 2,\n",
       " (148, 188): 1,\n",
       " (176, 234): 2,\n",
       " (185, 237): 3,\n",
       " (140, 168): 2,\n",
       " (49, 56): 1,\n",
       " (56, 32): 1,\n",
       " (140, 234): 1,\n",
       " (188, 237): 1,\n",
       " (158, 136): 4,\n",
       " (49, 57): 1,\n",
       " (57, 32): 1,\n",
       " (155, 148): 1,\n",
       " (154, 180): 2,\n",
       " (152, 129): 4,\n",
       " (129, 234): 1,\n",
       " (179, 180): 1,\n",
       " (236, 186): 3,\n",
       " (186, 161): 2,\n",
       " (161, 236): 2,\n",
       " (165, 234): 1,\n",
       " (48, 32): 1,\n",
       " (156, 161): 1,\n",
       " (161, 234): 1,\n",
       " (176, 129): 1,\n",
       " (152, 149): 1,\n",
       " (149, 32): 1,\n",
       " (32, 76): 2,\n",
       " (76, 67): 1,\n",
       " (67, 75): 1,\n",
       " (75, 32): 1,\n",
       " (237, 157): 1,\n",
       " (157, 145): 1,\n",
       " (145, 234): 1,\n",
       " (129, 157): 1,\n",
       " (157, 235): 1,\n",
       " (130, 184): 1,\n",
       " (236, 135): 2,\n",
       " (135, 188): 2,\n",
       " (169, 148): 2,\n",
       " (147, 234): 1,\n",
       " (236, 147): 1,\n",
       " (147, 176): 1,\n",
       " (139, 157): 2,\n",
       " (186, 144): 1,\n",
       " (178, 149): 1,\n",
       " (236, 178): 2,\n",
       " (178, 180): 1,\n",
       " (235, 145): 1,\n",
       " (145, 152): 1,\n",
       " (235, 137): 1,\n",
       " (137, 152): 1,\n",
       " (150, 165): 3,\n",
       " (164, 128): 1,\n",
       " (50, 50): 1,\n",
       " (181, 172): 1,\n",
       " (184, 237): 1,\n",
       " (178, 168): 1,\n",
       " (164, 44): 1,\n",
       " (160, 237): 1,\n",
       " (150, 137): 1,\n",
       " (137, 237): 1,\n",
       " (48, 51): 1,\n",
       " (51, 93): 1,\n",
       " (93, 91): 2,\n",
       " (48, 52): 1,\n",
       " (52, 93): 1,\n",
       " (10, 234): 1,\n",
       " (235, 136): 1,\n",
       " (136, 136): 1,\n",
       " (149, 158): 2,\n",
       " (158, 236): 2,\n",
       " (165, 153): 1,\n",
       " (153, 237): 1,\n",
       " (131, 235): 2,\n",
       " (139, 136): 1,\n",
       " (141, 44): 1,\n",
       " (169, 32): 1,\n",
       " (236, 176): 1,\n",
       " (176, 190): 1,\n",
       " (190, 234): 1,\n",
       " (158, 173): 1,\n",
       " (160, 164): 1,\n",
       " (167, 132): 1,\n",
       " (159, 191): 1,\n",
       " (191, 32): 1,\n",
       " (48, 53): 1,\n",
       " (53, 93): 1,\n",
       " (48, 54): 1,\n",
       " (54, 93): 1,\n",
       " (93, 32): 1,\n",
       " (133, 237): 1,\n",
       " (151, 132): 1,\n",
       " (178, 173): 1,\n",
       " (130, 156): 1,\n",
       " (129, 188): 2,\n",
       " (185, 156): 2,\n",
       " (168, 234): 1,\n",
       " (176, 236): 1,\n",
       " (165, 235): 1,\n",
       " (160, 165): 1,\n",
       " (132, 152): 1,\n",
       " (140, 144): 2,\n",
       " (128, 44): 1,\n",
       " (76, 80): 1,\n",
       " (80, 76): 1,\n",
       " (76, 32): 1,\n",
       " (137, 235): 1,\n",
       " (179, 128): 1,\n",
       " (236, 161): 1,\n",
       " (161, 177): 1,\n",
       " (152, 181): 1,\n",
       " (236, 133): 2,\n",
       " (133, 152): 2,\n",
       " (176, 148): 2,\n",
       " (148, 234): 2,\n",
       " (234, 190): 2,\n",
       " (190, 188): 2,\n",
       " (153, 152): 1,\n",
       " (167, 164): 1,\n",
       " (235, 191): 1,\n",
       " (191, 144): 1,\n",
       " (137, 44): 1,\n",
       " (184, 46): 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find byte (or int here) pair statistics\n",
    "def byte_pair(tokens):\n",
    "    pairs = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        pairs[pair] = pairs.get(pair, 0) + 1\n",
    "    return pairs\n",
    "\n",
    "pairs = byte_pair(tokens)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(176, (32, 236)),\n",
       " (141, (236, 157)),\n",
       " (112, (32, 235)),\n",
       " (66, (156, 32)),\n",
       " (60, (157, 180)),\n",
       " (60, (32, 237)),\n",
       " (52, (237, 149)),\n",
       " (46, (132, 32)),\n",
       " (40, (236, 160)),\n",
       " (39, (235, 147))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by the most frequent byte pair occurances\n",
    "sorted(((v, k) for k,v in pairs.items()), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (32, 236) into a new token 256\n",
      "merging (236, 157) into a new token 257\n",
      "merging (32, 235) into a new token 258\n",
      "merging (32, 237) into a new token 259\n",
      "merging (257, 180) into a new token 260\n",
      "merging (237, 149) into a new token 261\n",
      "merging (32, 234) into a new token 262\n",
      "merging (235, 138) into a new token 263\n",
      "merging (235, 161) into a new token 264\n",
      "merging (156, 256) into a new token 265\n",
      "merging (257, 132) into a new token 266\n",
      "---------------------------\n",
      "merging (32, 236) into a new token 256\n",
      "merging (236, 157) into a new token 257\n",
      "merging (32, 235) into a new token 258\n",
      "merging (156, 32) into a new token 259\n",
      "merging (157, 180) into a new token 260\n",
      "merging (32, 237) into a new token 261\n",
      "merging (237, 149) into a new token 262\n",
      "merging (132, 32) into a new token 263\n",
      "merging (236, 160) into a new token 264\n",
      "merging (235, 147) into a new token 265\n",
      "merging (235, 161) into a new token 266\n"
     ]
    }
   ],
   "source": [
    "# merge byte pairs into new token\n",
    "def merge_tokens(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurrences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids): # iterate through all tokens\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "def merge_byte_pairs(ids, vocab_size):\n",
    "    # Each merge iteration will increase total voacb size by 1\n",
    "    vocab_idx = 256\n",
    "    merge_iter = vocab_size - vocab_idx\n",
    "    merges = {}  # (int, int) -> int\n",
    "    for _ in range(merge_iter):\n",
    "        stats = byte_pair(ids)\n",
    "        pair = max(stats, key=stats.get)\n",
    "        print(f\"merging {pair} into a new token {vocab_idx}\")\n",
    "        ids = merge_tokens(ids, pair, vocab_idx)\n",
    "        merges[pair] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "        \n",
    "    return ids, merges\n",
    "\n",
    "# This method prevents calling byte_pair every iterations\n",
    "def merge_byte_pairs_fast(ids, vocab_size):\n",
    "    # Each merge iteration will increase total voacb size by 1\n",
    "    vocab_idx = 256\n",
    "    merge_iter = vocab_size - vocab_idx\n",
    "    merges = {}  # (int, int) -> int\n",
    "    stats = byte_pair(ids)\n",
    "    pairs = sorted(stats.items(), key=lambda x: x[1], reverse=True)[:merge_iter]\n",
    "\n",
    "    for pair, _ in pairs:\n",
    "        print(f\"merging {pair} into a new token {vocab_idx}\")\n",
    "        ids = merge_tokens(ids, pair, vocab_idx)\n",
    "        merges[pair] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "            \n",
    "    return ids, merges\n",
    "\n",
    "# Final vocab size is 267\n",
    "vocab_size = 267\n",
    "\n",
    "# Call the new function to perform the merging\n",
    "ids, merges = merge_byte_pairs(tokens, vocab_size)\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Call the fast version of merging by calling pair stats only once\n",
    "fids, fmerges = merge_byte_pairs_fast(tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 4112\n",
      "BPE encoded tokens length: 3389\n",
      "compression ratio: 1.21X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"BPE encoded tokens length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 4112\n",
      "Fast BPE encoded tokens length: 3492\n",
      "Fast compression ratio: 1.18X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"Fast BPE encoded tokens length:\", len(fids))\n",
    "print(f\"Fast compression ratio: {len(tokens) / len(fids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: tokenization step is independent to the pretraining LLM model\n",
    "# raw text (unicode code point) ---- (tokenizer) ---> token sequence -> LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(262, b' \\xea'),\n",
       " (263, b'\\xeb\\x8a'),\n",
       " (264, b'\\xeb\\xa1'),\n",
       " (265, b'\\x9c \\xec'),\n",
       " (266, b'\\xec\\x9d\\x84')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding: given token sequence, convert into raw text\n",
    "\n",
    "vocab = {i: bytes([i]) for i in range(256)} # original bytes\n",
    "# merged tokens\n",
    "for (i1, i2), tokenidx in merges.items(): \n",
    "    vocab[tokenidx] = vocab[i1] + vocab[i2]\n",
    "list(vocab.items())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ní˜ì´ì»¤ëŠ” ê²Œì„ ë‚´ï¿½'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(encodedtokens):\n",
    "    tokens = b\"\".join(vocab[i] for i in encodedtokens) # 1) token -> unicode byte\n",
    "    # default errors is \"strict\", replace prints invalid text instead without raising the error\n",
    "    # not all bytes are utf-8 decodable because of utf encoding rules\n",
    "    return tokens.decode(\"utf-8\", errors=\"replace\") # 2) unicode byte -> human readable text\n",
    "\n",
    "# ï¿½ is not decodable token\n",
    "decode(ids[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[237, 142, 152, 260, 236, 187, 164, 262, 184, 176, 236, 138, 181, 236, 136, 173, 235, 176, 176]\n"
     ]
    }
   ],
   "source": [
    "# encoding: given raw text, convert into token sequences\n",
    "\n",
    "def encode(text, merges):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "  # tokens less than length 2 cannot be merged\n",
    "  while len(tokens) >= 2: \n",
    "    # find byte pairs to be merged dict((int, int): int)\n",
    "    pairs = byte_pair(tokens)\n",
    "    # Find the pair with the minimum value (=token) in the merges dictionary\n",
    "    # Recall merges = ((token1, token2): newtoken above value 255)\n",
    "    # Because early merges should be conducted first\n",
    "    pair = min(pairs, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge_tokens(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"í˜ì´ì»¤ ê¸°ìŠµìˆ­ë°°\", merges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encode() missing 1 required positional argument: 'merges'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# encode and decode are inverse process\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m decode(\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mìì¥ë©´ ì§¬ë½• íƒ•ìˆ˜ìœ¡ ê¹í’ê¸°\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: encode() missing 1 required positional argument: 'merges'"
     ]
    }
   ],
   "source": [
    "# encode and decode are inverse process\n",
    "decode(encode(\"ìì¥ë©´ ì§¬ë½• íƒ•ìˆ˜ìœ¡ ê¹í’ê¸°\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex patterns used in GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"'re\", ' learning', ' NLP', ',', ' right', '?', ' I', \"'ve\", ' got', ' 10', ' tips', ' for', ' you', '!', '   ']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "# from gpt-2/src/encoder.py Encoder.pat\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    " \n",
    "print(re.findall(gpt2pat, \"You're learning NLP, right? I've got 10 tips for you!   \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **1. Matches Contractions (`'s|'t|'re|'ve|'m|'ll|'d`)**\n",
    "`'re` â†’ Matches `\"You're\"`.\n",
    "`'ve` â†’ Matches `\"I've\"`.\n",
    "\n",
    "###### **2. Matches Words (` ?\\p{L}+`)**\n",
    "`\"You\"`, `\"learning\"`, `\"NLP\"`, `\"right\"`, `\"I\"`, `\"got\"`, `\"tips\"`, `\"for\"`, `\"you\"`.\n",
    "\n",
    "######  **3. Matches Numbers (` ?\\p{N}+`)**\n",
    "`\"10\"`.\n",
    "\n",
    "###### **4. Matches Punctuation (` ?[^\\s\\p{L}\\p{N}]+`)**\n",
    "`\",\"`, `\"?\"`, `\"!\"`.\n",
    "\n",
    "###### **5. Matches Trailing Whitespace (`\\s+(?!\\S)`)**\n",
    "`\"   \"` (at the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# tiktoken library fast BPE tokeniser for openai models\n",
    "%pip install -q tiktoken\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 770, 318, 257, 2829, 1332, 284, 8996, 11241, 1634, 10064, 1022, 1180, 402, 11571, 3858]\n",
      "[262, 1115, 374, 264, 4382, 1296, 311, 9616, 4037, 2065, 15174, 1990, 2204, 480, 2898, 4595]\n",
      "[271, 1328, 382, 261, 4705, 1746, 316, 12221, 6602, 2860, 15142, 2870, 2647, 174803, 6009]\n"
     ]
    }
   ],
   "source": [
    "# Pattens are in https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    This is a simple test to compare tokenization strategies between different GPT types\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    This is a simple test to compare tokenization strategies between different GPT types\"))\n",
    "\n",
    "# GPT-4o (merges spaces, efficient token size)\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "print(enc.encode(\"    This is a simple test to compare tokenization strategies between different GPT types\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-19 14:53:44--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [application/octet-stream]\n",
      "Saving to: â€˜vocab.bpeâ€™\n",
      "\n",
      "vocab.bpe           100%[===================>] 445.62K   511KB/s    in 0.9s    \n",
      "\n",
      "2024-12-19 14:53:45 (511 KB/s) - â€˜vocab.bpeâ€™ saved [456318/456318]\n",
      "\n",
      "--2024-12-19 14:53:45--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: â€˜encoder.jsonâ€™\n",
      "\n",
      "encoder.json        100%[===================>]   1018K   744KB/s    in 1.4s    \n",
      "\n",
      "2024-12-19 14:53:48 (744 KB/s) - â€˜encoder.jsonâ€™ saved [1042301/1042301]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from gpt2 get_encoder func https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "# vocab.bpe: merges ((token1, token2) -> merged token id)\n",
    "# encoder.json: vocab (token integer -> byte)\n",
    "\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) \n",
    "\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 5 merges: [('Ä regress', 50252), ('Ä Collider', 50253), ('Ä informants', 50254), ('Ä gazed', 50255), ('<|endoftext|>', 50256)]\n",
      "Final 5 vocabs: [('om', 'inated'), ('Ä reg', 'ress'), ('Ä Coll', 'ider'), ('Ä inform', 'ants'), ('Ä g', 'azed')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final 5 merges: {[(k, v) for k, v in encoder.items()][-5:]}\")\n",
    "print(f\"Final 5 vocabs: {bpe_merges[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of encoder: 50257\n",
      "lenght of vocab: 50000\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of encoder: {len(encoder)}\") # 256 raw byte tokens + 50000 merges + 1 special token\n",
    "print(f\"lenght of vocab: {len(bpe_merges)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token\n",
    "# token used to signal end of document\n",
    "encoder[\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: gpt-4o have 2 special tokens\n",
    "# can be found at https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L100C5-L100C62\n",
    "# special_tokens = {ENDOFTEXT: 199999, ENDOFPROMPT: 200018}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.merges = {}  # (int, int) -> int\n",
    "        self.utf_size = 256 # utf encoding size (before merging)\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        merge_iter = vocab_size - self.utf_size\n",
    "        for i in range(merge_iter):\n",
    "            stats = byte_pair(tokens)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            if verbose:\n",
    "                print(f\"merging {pair} into a new token {self.utf_size + i}\")\n",
    "            tokens = merge_tokens(tokens, pair, self.utf_size + i)\n",
    "            self.merges[pair] = self.utf_size + i\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "        while len(tokens) >= 2: \n",
    "            pairs = byte_pair(tokens)\n",
    "            pair = min(pairs, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = merge_tokens(tokens, pair, idx)\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        vocab = {i: bytes([i]) for i in range(256)} \n",
    "        for (i1, i2), tokenidx in self.merges.items(): \n",
    "            vocab[tokenidx] = vocab[i1] + vocab[i2]\n",
    "        \n",
    "        tokens = b\"\".join(vocab[i] for i in ids)  \n",
    "        return tokens.decode(\"utf-8\", errors=\"replace\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "basictokenizer = BasicTokenizer()\n",
    "\n",
    "with open(\"taylor.txt\", \"r\") as file:\n",
    "    txt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "basictokenizer.train(txt, 285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 97, 107, 101, 114, 32, 105, 115, 32, 116, 104, 101, 32, 98, 101, 115, 116, 32, 236, 136, 173, 235, 176, 176, 237, 149, 160, 32, 236, 139, 156, 234, 176, 132, 236, 157, 180, 236, 151, 144, 236, 154, 148]\n",
      "Faker is the best ìˆ­ë°°í•  ì‹œê°„ì´ì—ìš”\n"
     ]
    }
   ],
   "source": [
    "enc = basictokenizer.encode(\"Faker is the best ìˆ­ë°°í•  ì‹œê°„ì´ì—ìš”\")\n",
    "print(enc)\n",
    "\n",
    "dec = basictokenizer.decode(enc)\n",
    "print(dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencepiece is used in Llama and Mistral\n",
    "# they run bpe on unicode code points directly\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"In 2024, the world saw dramatic changesâ€”politics, tech, and culture shifted. Social media platforms like X (formerly Twitter) & Meta thrived, while others faltered. Passwords like Pa$$w0rd! became outdated; biometrics took over. Emojis ğŸ‰ & hashtags #Change2024 dominated conversations. At the heart of innovation, AI models (e.g., ChatGPT) continued their evolution. Companies discussed ethics: Should AIs say â€œyesâ€ to every ğŸ› ï¸? Or â€œnoâ€? Meanwhile, everyday folks exclaimed, \\\"OMG! What a time to be alive! ğŸ˜….\\\" Data (1TB, 2TB) flowed rapidly, and files like report_v2_final.pdf defined workflows. In a fast-paced era, the question was simple: Where do we go next? ğŸŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: toy.txt\n",
      "  input_format: text\n",
      "  model_prefix: tok400\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 200000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 8\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: toy.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=665\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=66\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 91\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=20 all=403 active=337 piece=â–s\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=40 all=430 active=364 piece=TB\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=60 all=461 active=395 piece=and\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok400.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok400.vocab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# due to long history, sentencepiece has a lot of configurations\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization (popular config before LLM era, prefer not to touch in LLM era)\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment (important configs)\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules (simiilar to regex rules in tiktoken)\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist \n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2, # end of sentence\n",
    "  pad_id=-1, # -1, thus not use pad id\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['ed', 259],\n",
       " ['er', 260],\n",
       " ['â–t', 261],\n",
       " ['at', 262],\n",
       " ['an', 263],\n",
       " ['on', 264],\n",
       " ['or', 265],\n",
       " ['es', 266],\n",
       " ['he', 267],\n",
       " ['in', 268],\n",
       " ['li', 269],\n",
       " ['â–d', 270],\n",
       " ['â–e', 271],\n",
       " ['â–f', 272],\n",
       " ['â–w', 273],\n",
       " ['as', 274],\n",
       " ['hi', 275],\n",
       " ['â–c', 276],\n",
       " ['â–o', 277],\n",
       " ['â–s', 278],\n",
       " ['ion', 279],\n",
       " ['ver', 280],\n",
       " ['â–the', 281],\n",
       " ['al', 282],\n",
       " ['cs', 283],\n",
       " ['im', 284],\n",
       " ['ke', 285],\n",
       " ['le', 286],\n",
       " ['om', 287],\n",
       " ['â–(', 288],\n",
       " ['â–A', 289],\n",
       " ['â–a', 290],\n",
       " ['â–b', 291],\n",
       " ['â–li', 292],\n",
       " ['â–to', 293],\n",
       " ['â–like', 294],\n",
       " ['Ch', 295],\n",
       " ['In', 296],\n",
       " ['Me', 297],\n",
       " ['TB', 298],\n",
       " ['Wh', 299],\n",
       " ['am', 300],\n",
       " ['ay', 301],\n",
       " ['ec', 302],\n",
       " ['is', 303],\n",
       " ['ld', 304],\n",
       " ['lo', 305],\n",
       " ['ly', 306],\n",
       " ['mo', 307],\n",
       " ['no', 308],\n",
       " ['ol', 309],\n",
       " ['re', 310],\n",
       " ['ri', 311],\n",
       " ['ta', 312],\n",
       " ['ut', 313],\n",
       " ['â–&', 314],\n",
       " ['â–P', 315],\n",
       " ['â–S', 316],\n",
       " ['â–â€œ', 317],\n",
       " ['and', 318],\n",
       " ['ang', 319],\n",
       " ['fin', 320],\n",
       " ['for', 321],\n",
       " ['low', 322],\n",
       " ['ter', 323],\n",
       " ['â–AI', 324],\n",
       " ['â–In', 325],\n",
       " ['â–Me', 326],\n",
       " ['â–Wh', 327],\n",
       " ['ated', 328],\n",
       " ['form', 329],\n",
       " ['hile', 330],\n",
       " ['very', 331],\n",
       " ['â–and', 332],\n",
       " ['â–con', 333],\n",
       " ['â–', 334],\n",
       " ['e', 335],\n",
       " ['a', 336],\n",
       " ['t', 337],\n",
       " ['o', 338],\n",
       " ['i', 339],\n",
       " ['s', 340],\n",
       " ['d', 341],\n",
       " ['r', 342],\n",
       " ['l', 343],\n",
       " ['n', 344],\n",
       " ['h', 345],\n",
       " ['c', 346],\n",
       " ['f', 347],\n",
       " ['m', 348],\n",
       " ['w', 349],\n",
       " [',', 350],\n",
       " ['.', 351],\n",
       " ['v', 352],\n",
       " ['p', 353],\n",
       " ['u', 354],\n",
       " ['y', 355],\n",
       " ['2', 356],\n",
       " ['k', 357],\n",
       " ['g', 358],\n",
       " ['I', 359],\n",
       " ['T', 360],\n",
       " ['!', 361],\n",
       " ['(', 362],\n",
       " [')', 363],\n",
       " ['0', 364],\n",
       " ['?', 365],\n",
       " ['A', 366],\n",
       " ['C', 367],\n",
       " ['M', 368],\n",
       " ['P', 369],\n",
       " ['b', 370],\n",
       " ['\"', 371],\n",
       " ['$', 372],\n",
       " ['&', 373],\n",
       " ['4', 374],\n",
       " [':', 375],\n",
       " ['B', 376],\n",
       " ['G', 377],\n",
       " ['O', 378],\n",
       " ['S', 379],\n",
       " ['W', 380],\n",
       " ['_', 381],\n",
       " ['x', 382],\n",
       " ['â€œ', 383],\n",
       " ['â€', 384],\n",
       " ['#', 385],\n",
       " ['-', 386],\n",
       " ['1', 387],\n",
       " [';', 388],\n",
       " ['D', 389],\n",
       " ['E', 390],\n",
       " ['X', 391],\n",
       " ['j', 392],\n",
       " ['q', 393],\n",
       " ['â€”', 394],\n",
       " ['ï¸', 395],\n",
       " ['ğŸŒ', 396],\n",
       " ['ğŸ‰', 397],\n",
       " ['ğŸ˜…', 398],\n",
       " ['ğŸ› ', 399]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab # order: special token - byte tokens - merged tokens - raw codepoints tokens (in toy.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indivisual codepoint tokens should have low frequency (= rare)\n",
    "# character_coverage configs will let them not to be added to vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[334, 267, 343, 305, 334, 240, 145, 155, 239, 160, 183, 239, 190, 167, 334, 237, 187, 179, 239, 141, 184, 239, 139, 176, 238, 179, 179, 334, 239, 142, 159, 237, 179, 135, 239, 161, 136, 238, 142, 139, 238, 142, 167]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"hello í˜ì´ì»¤ ê¸°ìŠµìˆ­ë°° ì‹œê°„ì…ë‹ˆë‹¤\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–', 'he', 'l', 'lo', 'â–', '<0xED>', '<0x8E>', '<0x98>', '<0xEC>', '<0x9D>', '<0xB4>', '<0xEC>', '<0xBB>', '<0xA4>', 'â–', '<0xEA>', '<0xB8>', '<0xB0>', '<0xEC>', '<0x8A>', '<0xB5>', '<0xEC>', '<0x88>', '<0xAD>', '<0xEB>', '<0xB0>', '<0xB0>', 'â–', '<0xEC>', '<0x8B>', '<0x9C>', '<0xEA>', '<0xB0>', '<0x84>', '<0xEC>', '<0x9E>', '<0x85>', '<0xEB>', '<0x8B>', '<0x88>', '<0xEB>', '<0x8B>', '<0xA4>']\n"
     ]
    }
   ],
   "source": [
    "# Since korean characters are not trained (not included in toy.txt)\n",
    "# As byte_fallback=True, it encodes to utf-8 <0x..> tokens\n",
    "# If byte_fallback=false, it maps to <unk> token 0\n",
    "\n",
    "# Space switches spaces in to '_' (including first space - config add_dummy_prefix=True)\n",
    "# Let world in \"hello world\" and \"world\" have same token (\"world\" becomes \" world\")\n",
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding appropriate vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing vocab_size will increase computations\n",
    "# Because the final layers of LLM modles are logits/probs outputing layer nn.Linear(embed_size, vocab_size)\n",
    "# Large vocab_size may under train Embedding vector of some vocabs in nn.Embedding(vocab_size, embed_size)\n",
    "# When adding new tokens from existing model, initialize new vocabs' embedding vecotrs randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of other modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The architecture stays the same!\n",
    "# e.g. image - VQGAN, video - sora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
