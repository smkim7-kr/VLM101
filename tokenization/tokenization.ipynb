{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization tutorial\n",
    "(Reference : https://www.youtube.com/watch?v=zduSFxRajkE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54620, 44397, 50612, 32, 51339, 50500, 54644, 50836, 32, 40, 10355, 10357, 10359, 41, 32, 65295, 20154, 9685, 8255, 8255, 9685, 20154, 65340]\n"
     ]
    }
   ],
   "source": [
    "# ord() converts character to unicode code points\n",
    "# Unicode is updated frequently, about 140000 characters supported\n",
    "# Cannot use this nativaly since too many vocabs & and update frequently, thus not stable\n",
    "str = \"한국어 좋아해요 (⡳⡵⡷) ／人◕‿‿◕人＼\"\n",
    "print([ord(x) for x in str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  한국어 좋아해요 (⡳⡵⡷) ／人◕‿‿◕人＼\n",
      "String length:  23\n",
      "Encoded:  [237, 149, 156, 234, 181, 173, 236, 150, 180, 32, 236, 162, 139, 236, 149, 132, 237, 149, 180, 236, 154, 148, 32, 40, 226, 161, 179, 226, 161, 181, 226, 161, 183, 41, 32, 239, 188, 143, 228, 186, 186, 226, 151, 149, 226, 128, 191, 226, 128, 191, 226, 151, 149, 228, 186, 186, 239, 188, 188]\n",
      "Encoded length:  59\n"
     ]
    }
   ],
   "source": [
    "# Unicode can encoded using into binary data\n",
    "# Types: utf-8 (mostly used), utf-16, utf-32\n",
    "print(\"String: \", str)\n",
    "print(\"String length: \", len(str))\n",
    "tokens = list(str.encode(\"utf-8\"))\n",
    "print(\"Encoded: \", tokens)\n",
    "print(\"Encoded length: \", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded (utf-32):  [255, 254, 0, 0, 92, 213, 0, 0, 109, 173, 0, 0, 180, 197, 0, 0, 32, 0, 0, 0, 139, 200, 0, 0, 68, 197, 0, 0, 116, 213, 0, 0, 148, 198, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 115, 40, 0, 0, 117, 40, 0, 0, 119, 40, 0, 0, 41, 0, 0, 0, 32, 0, 0, 0, 15, 255, 0, 0, 186, 78, 0, 0, 213, 37, 0, 0, 63, 32, 0, 0, 63, 32, 0, 0, 213, 37, 0, 0, 186, 78, 0, 0, 60, 255, 0, 0]\n",
      "Encoded length (utf-32):  96\n"
     ]
    }
   ],
   "source": [
    "# Other utf types are not efficient in many cases\n",
    "# Here utf-32 includes several \"0\" token, thus longer token length compare to utf-8 encoding\n",
    "print(\"Encoded (utf-32): \", list(str.encode(\"utf-32\")))\n",
    "print(\"Encoded length (utf-32): \", len(list(str.encode(\"utf-32\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte Pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte pair encoding replaces frequent byte pair into a single byte\n",
    "# e.g. xyabxyxy => zabzz where z = xy\n",
    "# This reduces the length of token sequences\n",
    "# Transformers with fixed context length, it can effectively attend larger context (as multiple tokens' context are squeezed into single token)\n",
    "str = \"\"\"\n",
    "페이커는 게임 내부적으로 현대 미드라이너의 개념을 정립한 선수로 평가된다. 페이커의 등장은 고전적 1세대 미드라이너 시대의 종언을 알렸다. 시즌 2까지 미드 라이너는 무리하지 않고 파밍을 하면서 성장하고 주로 필요한 순간에만 합류를 했는데, 2013 시즌부터 등장한 페이커는 상대 라이너를 끊임없이 압박하면서 이득을 취하는 플레이를 시작했다.\n",
    "유명 아마추어 게이머인 도파는 \"미드 라이너로서 최고의 플레이는 상대방을 극단적으로 압박해 적 정글을 불러들일 수밖에 없는 상황을 만들고 2:1 상황을 만든 후에 죽지 않는 것이다. 그리고 이 2:1 드래블링을 최초로 시작한 것이 페이커였고 이를 최적화시켜 완성한 사람이 루키였다.\"라고 평가했다.[101]\n",
    "즉 상대하는 입장에서 적 정글을 불러들일 수밖에 없는 상황을 만든 뒤에 적 정글이 갱이나 견제를 해오면 본인은 어그로를 끌면서 팀적으로 상대 정글의 위치를 이용해 다른 라인에 힘을 실어주기 혹은 역갱을 성공시켜 그에 따른 스노우볼을 굴리거나 최적의 상황으로는 1대2 역관광을 해 게임을 터뜨리는 것이다. 반대로 본인에게 견제가 오지 않는다면, 라인 주도권을 이용해 한발 빠른 합류와 타 라인에 대한 견제를 넣거나 아예 미드 라인에서 솔킬을 내버리고 폭파시켜버리는 등, 다시 말해 적 미드 정글을 자기 마음대로 컨트롤하여 게임을 터뜨리는, 소위 스포츠에서 통칭되는 '크랙 플레이'의 정점에 가까웠던 선수였다.\n",
    "실제로 이 2대1 드리블링이 가능한 지 여부는 이후 미드, 탑 선수 중 S급과 A급을 가르는 중요한 지표중 하나가 되었다. 페이커가 최초로 시작하고 루키가 최적화시킨 후 2015 시즌부터 본격적으로 마린, 스맵, 큐베, 칸, 더샤이, 비디디, 쵸비 등 속칭 \"S급 선수\"들이 대거 등장하기 시작했다. [102]\n",
    "실제로 이러한 플레이의 과정에서 나오는 페이커 특유의 외줄타기, 솔로 킬, 넓은 챔프폭, 슈퍼 플레이는 사람들을 열광하게 해 경기 외적으로 막 인기를 얻기 시작하던 리그 오브 레전드와 롤챔스의 유명세를 더욱 증폭시키는 기폭제 역할을 하기도 했고 르블랑, 아리, 제드 등의 암살자 챔피언의 인기가 급상승하기도 했다.\n",
    "이 패러다임은 2018 시즌까지 미드라이너의 유일한 크랙 플레이로 남아있었다. 하지만 2019 시즌 탁월한 운영과 로밍으로 승부를 보는 도인비와 캡스의 등장과 2020 시즌 육각형 플레이로 LCK 암흑기를 끝낸 쇼메이커 이후로 더 이상 상대 미드를 극한으로 압박하지 않고 맵을 넓게 쓰는 방식으로도 캐리할 수 있다는 해법이 나오면서 전체적인 미드 플레이 스타일은 크게 이 둘로 나뉘게 되었다. 그리고 미드 라이너들의 상향평준화와 2022 시즌 내구도 패치로 인해 더 이상 프로 레벨에서 상대방을 압도하기가 힘들어지자 도인비와 캡스, 쇼메이커 스타일이 유행하기 시작했다.[103][104]\n",
    "그 외에도 단순히 내 눈앞에 서 있는 상대방을 도륙하는 것만이 아니라 와드를 박는 위치나 타이밍, 이를 이용한 상대방 미드와 정글 위치 찾기 등의 지능적 플레이 역시 솔로랭크 리플레이와 관전 등으로 알려진 것만 여럿 된다.[105][106] 종합적으로 페이커는 게임 내외에 엄청난 영향을 끼친 선수로, 단기적으로나마 페이커 급의 영향력을 끼친 선수는 서포터를 넘어 아예 롤판의 기본 운영을 정립한 마타, LPL 탑 라인에 급변을 불러와 슈퍼스타에 등극한 더샤이, 커리어는 앞선 선수들에 비해 부족하지만 팀의 1옵션으로 포지션의 인식을 뒤바꾼 롤판의 임요환 매드라이프정도 뿐이다. 즉, 리그와 게임을 바꾼 개인.\n",
    "\"\"\"\n",
    "tokens = list(str.encode(\"utf-8\"))\n",
    "tokens = list(map(int, tokens)) # for convinience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  4112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10, 237, 142, 152, 236, 157, 180, 236, 187, 164]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Length: \", len(tokens))\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(10, 237): 1,\n",
       " (237, 142): 8,\n",
       " (142, 152): 8,\n",
       " (152, 236): 23,\n",
       " (236, 157): 141,\n",
       " (157, 180): 60,\n",
       " (180, 236): 29,\n",
       " (236, 187): 12,\n",
       " (187, 164): 11,\n",
       " (164, 235): 16,\n",
       " (235, 138): 33,\n",
       " (138, 148): 31,\n",
       " (148, 32): 30,\n",
       " (32, 234): 35,\n",
       " (234, 178): 21,\n",
       " (178, 140): 11,\n",
       " (140, 236): 14,\n",
       " (236, 158): 27,\n",
       " (158, 132): 8,\n",
       " (132, 32): 46,\n",
       " (32, 235): 112,\n",
       " (235, 130): 17,\n",
       " (130, 180): 6,\n",
       " (180, 235): 31,\n",
       " (235, 182): 9,\n",
       " (182, 128): 6,\n",
       " (128, 236): 18,\n",
       " (236, 160): 40,\n",
       " (160, 129): 17,\n",
       " (129, 236): 14,\n",
       " (236, 156): 23,\n",
       " (156, 188): 13,\n",
       " (188, 235): 17,\n",
       " (235, 161): 37,\n",
       " (161, 156): 33,\n",
       " (156, 32): 66,\n",
       " (32, 237): 60,\n",
       " (237, 152): 3,\n",
       " (152, 132): 1,\n",
       " (132, 235): 11,\n",
       " (235, 140): 17,\n",
       " (140, 128): 19,\n",
       " (128, 32): 35,\n",
       " (235, 175): 12,\n",
       " (175, 184): 12,\n",
       " (184, 235): 22,\n",
       " (235, 147): 39,\n",
       " (147, 156): 18,\n",
       " (156, 235): 25,\n",
       " (235, 157): 15,\n",
       " (157, 188): 20,\n",
       " (188, 236): 21,\n",
       " (235, 132): 11,\n",
       " (132, 136): 7,\n",
       " (136, 236): 17,\n",
       " (157, 152): 21,\n",
       " (152, 32): 31,\n",
       " (234, 176): 17,\n",
       " (176, 156): 3,\n",
       " (235, 133): 2,\n",
       " (133, 144): 1,\n",
       " (144, 236): 10,\n",
       " (157, 132): 32,\n",
       " (32, 236): 176,\n",
       " (160, 149): 11,\n",
       " (149, 235): 8,\n",
       " (235, 166): 18,\n",
       " (166, 189): 2,\n",
       " (189, 237): 2,\n",
       " (237, 149): 52,\n",
       " (149, 156): 16,\n",
       " (236, 132): 25,\n",
       " (132, 160): 8,\n",
       " (160, 236): 13,\n",
       " (236, 136): 12,\n",
       " (136, 152): 10,\n",
       " (152, 235): 24,\n",
       " (237, 143): 10,\n",
       " (143, 137): 3,\n",
       " (137, 234): 3,\n",
       " (176, 128): 11,\n",
       " (128, 235): 17,\n",
       " (235, 144): 5,\n",
       " (144, 156): 2,\n",
       " (235, 139): 25,\n",
       " (139, 164): 24,\n",
       " (164, 46): 16,\n",
       " (46, 32): 9,\n",
       " (164, 236): 23,\n",
       " (147, 177): 10,\n",
       " (177, 236): 10,\n",
       " (158, 165): 6,\n",
       " (165, 236): 5,\n",
       " (157, 128): 6,\n",
       " (234, 179): 18,\n",
       " (179, 160): 13,\n",
       " (160, 132): 4,\n",
       " (132, 236): 24,\n",
       " (129, 32): 8,\n",
       " (32, 49): 3,\n",
       " (49, 236): 2,\n",
       " (132, 184): 2,\n",
       " (136, 32): 5,\n",
       " (236, 139): 26,\n",
       " (139, 156): 21,\n",
       " (236, 162): 2,\n",
       " (162, 133): 2,\n",
       " (133, 236): 3,\n",
       " (236, 150): 9,\n",
       " (150, 184): 2,\n",
       " (184, 236): 19,\n",
       " (236, 149): 20,\n",
       " (149, 140): 2,\n",
       " (140, 235): 22,\n",
       " (235, 160): 15,\n",
       " (160, 184): 1,\n",
       " (156, 236): 27,\n",
       " (236, 166): 10,\n",
       " (166, 140): 7,\n",
       " (140, 32): 14,\n",
       " (32, 50): 10,\n",
       " (50, 234): 1,\n",
       " (234, 185): 3,\n",
       " (185, 140): 3,\n",
       " (236, 167): 14,\n",
       " (167, 128): 13,\n",
       " (136, 235): 23,\n",
       " (235, 172): 1,\n",
       " (172, 180): 1,\n",
       " (166, 172): 15,\n",
       " (172, 237): 5,\n",
       " (149, 152): 20,\n",
       " (149, 138): 4,\n",
       " (138, 234): 2,\n",
       " (160, 32): 15,\n",
       " (237, 140): 9,\n",
       " (140, 140): 3,\n",
       " (235, 176): 18,\n",
       " (176, 141): 3,\n",
       " (141, 236): 3,\n",
       " (235, 169): 8,\n",
       " (169, 180): 6,\n",
       " (132, 156): 12,\n",
       " (132, 177): 3,\n",
       " (165, 237): 5,\n",
       " (152, 234): 10,\n",
       " (236, 163): 4,\n",
       " (163, 188): 3,\n",
       " (149, 132): 7,\n",
       " (236, 154): 10,\n",
       " (154, 148): 3,\n",
       " (148, 237): 8,\n",
       " (136, 156): 2,\n",
       " (156, 234): 3,\n",
       " (176, 132): 1,\n",
       " (236, 151): 36,\n",
       " (151, 144): 21,\n",
       " (144, 235): 4,\n",
       " (235, 167): 20,\n",
       " (167, 140): 8,\n",
       " (149, 169): 3,\n",
       " (169, 235): 2,\n",
       " (235, 165): 24,\n",
       " (165, 152): 2,\n",
       " (165, 188): 16,\n",
       " (188, 32): 25,\n",
       " (237, 150): 11,\n",
       " (150, 136): 7,\n",
       " (148, 235): 8,\n",
       " (235, 141): 8,\n",
       " (141, 176): 1,\n",
       " (176, 44): 3,\n",
       " (44, 32): 22,\n",
       " (50, 48): 7,\n",
       " (48, 49): 5,\n",
       " (49, 51): 1,\n",
       " (51, 32): 1,\n",
       " (128, 237): 7,\n",
       " (237, 132): 5,\n",
       " (132, 176): 5,\n",
       " (176, 32): 10,\n",
       " (236, 131): 18,\n",
       " (131, 129): 17,\n",
       " (129, 235): 8,\n",
       " (235, 129): 5,\n",
       " (129, 138): 1,\n",
       " (138, 236): 1,\n",
       " (151, 134): 3,\n",
       " (134, 236): 1,\n",
       " (180, 32): 27,\n",
       " (149, 149): 4,\n",
       " (176, 149): 4,\n",
       " (149, 237): 3,\n",
       " (147, 157): 1,\n",
       " (157, 236): 3,\n",
       " (236, 183): 1,\n",
       " (183, 168): 1,\n",
       " (168, 237): 4,\n",
       " (237, 148): 14,\n",
       " (148, 140): 10,\n",
       " (160, 136): 12,\n",
       " (158, 145): 7,\n",
       " (145, 237): 7,\n",
       " (46, 10): 4,\n",
       " (10, 236): 5,\n",
       " (156, 160): 5,\n",
       " (160, 235): 4,\n",
       " (235, 170): 2,\n",
       " (170, 133): 2,\n",
       " (133, 32): 1,\n",
       " (167, 136): 5,\n",
       " (236, 182): 1,\n",
       " (182, 148): 1,\n",
       " (148, 236): 11,\n",
       " (150, 180): 6,\n",
       " (235, 168): 1,\n",
       " (168, 184): 1,\n",
       " (157, 184): 16,\n",
       " (184, 32): 10,\n",
       " (235, 143): 12,\n",
       " (143, 132): 12,\n",
       " (132, 237): 5,\n",
       " (32, 34): 2,\n",
       " (34, 235): 3,\n",
       " (236, 181): 7,\n",
       " (181, 156): 6,\n",
       " (176, 169): 5,\n",
       " (169, 236): 10,\n",
       " (234, 183): 10,\n",
       " (183, 185): 3,\n",
       " (185, 235): 2,\n",
       " (139, 168): 3,\n",
       " (168, 236): 5,\n",
       " (149, 180): 10,\n",
       " (149, 234): 6,\n",
       " (234, 184): 29,\n",
       " (184, 128): 6,\n",
       " (182, 136): 3,\n",
       " (235, 159): 6,\n",
       " (159, 172): 5,\n",
       " (172, 235): 14,\n",
       " (147, 164): 8,\n",
       " (176, 150): 2,\n",
       " (150, 236): 2,\n",
       " (144, 32): 15,\n",
       " (134, 235): 2,\n",
       " (129, 237): 10,\n",
       " (237, 153): 8,\n",
       " (153, 169): 4,\n",
       " (164, 234): 3,\n",
       " (50, 58): 2,\n",
       " (58, 49): 2,\n",
       " (49, 32): 3,\n",
       " (147, 160): 2,\n",
       " (237, 155): 4,\n",
       " (155, 132): 4,\n",
       " (163, 189): 1,\n",
       " (189, 236): 1,\n",
       " (138, 235): 2,\n",
       " (178, 131): 5,\n",
       " (131, 236): 3,\n",
       " (183, 184): 7,\n",
       " (172, 234): 7,\n",
       " (235, 158): 7,\n",
       " (158, 152): 3,\n",
       " (235, 184): 4,\n",
       " (184, 148): 3,\n",
       " (167, 129): 2,\n",
       " (236, 180): 2,\n",
       " (180, 136): 2,\n",
       " (236, 152): 15,\n",
       " (152, 128): 3,\n",
       " (128, 234): 5,\n",
       " (153, 148): 3,\n",
       " (236, 188): 3,\n",
       " (188, 156): 3,\n",
       " (236, 153): 15,\n",
       " (153, 132): 1,\n",
       " (177, 237): 2,\n",
       " (236, 130): 3,\n",
       " (130, 172): 4,\n",
       " (158, 140): 2,\n",
       " (235, 163): 2,\n",
       " (163, 168): 2,\n",
       " (237, 130): 6,\n",
       " (130, 164): 3,\n",
       " (46, 34): 1,\n",
       " (188, 234): 2,\n",
       " (46, 91): 3,\n",
       " (91, 49): 6,\n",
       " (49, 48): 6,\n",
       " (49, 93): 1,\n",
       " (93, 10): 3,\n",
       " (166, 137): 2,\n",
       " (137, 32): 3,\n",
       " (158, 133): 1,\n",
       " (235, 146): 2,\n",
       " (146, 164): 2,\n",
       " (176, 177): 2,\n",
       " (130, 152): 9,\n",
       " (178, 172): 3,\n",
       " (172, 236): 8,\n",
       " (160, 156): 7,\n",
       " (152, 164): 5,\n",
       " (235, 179): 7,\n",
       " (179, 184): 4,\n",
       " (180, 234): 3,\n",
       " (129, 140): 1,\n",
       " (156, 132): 4,\n",
       " (236, 185): 9,\n",
       " (185, 152): 4,\n",
       " (154, 169): 3,\n",
       " (169, 237): 3,\n",
       " (165, 184): 3,\n",
       " (237, 158): 3,\n",
       " (184, 176): 17,\n",
       " (152, 185): 1,\n",
       " (185, 236): 2,\n",
       " (151, 173): 4,\n",
       " (173, 234): 2,\n",
       " (177, 234): 2,\n",
       " (179, 181): 1,\n",
       " (181, 236): 4,\n",
       " (235, 148): 3,\n",
       " (148, 176): 1,\n",
       " (176, 235): 13,\n",
       " (236, 138): 13,\n",
       " (138, 164): 9,\n",
       " (133, 184): 1,\n",
       " (154, 176): 1,\n",
       " (179, 188): 5,\n",
       " (234, 181): 2,\n",
       " (181, 180): 1,\n",
       " (234, 177): 3,\n",
       " (177, 176): 3,\n",
       " (49, 235): 1,\n",
       " (128, 50): 1,\n",
       " (50, 32): 2,\n",
       " (234, 180): 4,\n",
       " (180, 128): 2,\n",
       " (180, 145): 2,\n",
       " (145, 236): 2,\n",
       " (235, 156): 2,\n",
       " (156, 168): 2,\n",
       " (168, 235): 3,\n",
       " (176, 152): 1,\n",
       " (144, 234): 2,\n",
       " (180, 44): 3,\n",
       " (132, 234): 1,\n",
       " (234, 182): 1,\n",
       " (182, 140): 1,\n",
       " (235, 185): 6,\n",
       " (185, 160): 1,\n",
       " (153, 128): 10,\n",
       " (237, 131): 10,\n",
       " (131, 128): 7,\n",
       " (132, 163): 1,\n",
       " (163, 234): 1,\n",
       " (152, 136): 2,\n",
       " (236, 134): 5,\n",
       " (134, 148): 3,\n",
       " (235, 178): 5,\n",
       " (178, 132): 2,\n",
       " (143, 173): 4,\n",
       " (173, 237): 3,\n",
       " (177, 44): 1,\n",
       " (167, 144): 1,\n",
       " (144, 237): 1,\n",
       " (158, 144): 3,\n",
       " (157, 140): 1,\n",
       " (187, 168): 1,\n",
       " (237, 138): 2,\n",
       " (138, 184): 1,\n",
       " (161, 164): 4,\n",
       " (164, 237): 7,\n",
       " (151, 172): 3,\n",
       " (172, 32): 2,\n",
       " (148, 44): 2,\n",
       " (134, 140): 1,\n",
       " (143, 172): 3,\n",
       " (236, 184): 1,\n",
       " (184, 160): 1,\n",
       " (237, 134): 1,\n",
       " (134, 181): 1,\n",
       " (185, 173): 2,\n",
       " (173, 235): 2,\n",
       " (144, 152): 3,\n",
       " (32, 39): 1,\n",
       " (39, 237): 1,\n",
       " (237, 129): 5,\n",
       " (129, 172): 4,\n",
       " (158, 153): 2,\n",
       " (153, 32): 2,\n",
       " (180, 39): 1,\n",
       " (39, 236): 1,\n",
       " (149, 236): 3,\n",
       " (160, 144): 1,\n",
       " (236, 155): 2,\n",
       " (155, 160): 1,\n",
       " (141, 152): 2,\n",
       " (50, 235): 1,\n",
       " (128, 49): 1,\n",
       " (138, 165): 2,\n",
       " (180, 237): 3,\n",
       " (156, 44): 2,\n",
       " (131, 145): 2,\n",
       " (145, 32): 4,\n",
       " (236, 164): 5,\n",
       " (164, 145): 3,\n",
       " (32, 83): 1,\n",
       " (83, 234): 2,\n",
       " (184, 137): 6,\n",
       " (32, 65): 1,\n",
       " (65, 234): 1,\n",
       " (137, 236): 4,\n",
       " (165, 180): 2,\n",
       " (237, 145): 1,\n",
       " (145, 156): 1,\n",
       " (151, 136): 3,\n",
       " (156, 237): 4,\n",
       " (130, 168): 2,\n",
       " (168, 32): 1,\n",
       " (49, 53): 1,\n",
       " (53, 32): 1,\n",
       " (184, 234): 3,\n",
       " (178, 169): 1,\n",
       " (166, 176): 1,\n",
       " (167, 181): 2,\n",
       " (181, 44): 1,\n",
       " (129, 144): 1,\n",
       " (178, 160): 1,\n",
       " (160, 44): 1,\n",
       " (185, 184): 1,\n",
       " (184, 44): 1,\n",
       " (141, 148): 5,\n",
       " (131, 164): 2,\n",
       " (185, 132): 5,\n",
       " (148, 148): 2,\n",
       " (181, 184): 1,\n",
       " (177, 32): 2,\n",
       " (134, 141): 1,\n",
       " (173, 32): 1,\n",
       " (34, 83): 1,\n",
       " (152, 34): 1,\n",
       " (32, 91): 1,\n",
       " (48, 50): 3,\n",
       " (50, 93): 1,\n",
       " (164, 32): 4,\n",
       " (138, 185): 3,\n",
       " (153, 184): 4,\n",
       " (164, 132): 1,\n",
       " (172, 44): 2,\n",
       " (132, 147): 2,\n",
       " (147, 236): 1,\n",
       " (236, 177): 3,\n",
       " (177, 148): 3,\n",
       " (148, 132): 3,\n",
       " (173, 44): 1,\n",
       " (138, 136): 2,\n",
       " (136, 237): 3,\n",
       " (237, 141): 2,\n",
       " (141, 188): 2,\n",
       " (151, 180): 1,\n",
       " (178, 189): 1,\n",
       " (189, 234): 1,\n",
       " (167, 137): 1,\n",
       " (150, 187): 1,\n",
       " (187, 234): 1,\n",
       " (184, 140): 1,\n",
       " (154, 177): 1,\n",
       " (166, 157): 1,\n",
       " (157, 237): 1,\n",
       " (173, 236): 3,\n",
       " (176, 237): 1,\n",
       " (149, 160): 2,\n",
       " (136, 234): 1,\n",
       " (145, 44): 1,\n",
       " (149, 148): 2,\n",
       " (148, 188): 1,\n",
       " (176, 234): 2,\n",
       " (185, 237): 3,\n",
       " (140, 168): 2,\n",
       " (49, 56): 1,\n",
       " (56, 32): 1,\n",
       " (140, 234): 1,\n",
       " (188, 237): 1,\n",
       " (158, 136): 4,\n",
       " (49, 57): 1,\n",
       " (57, 32): 1,\n",
       " (155, 148): 1,\n",
       " (154, 180): 2,\n",
       " (152, 129): 4,\n",
       " (129, 234): 1,\n",
       " (179, 180): 1,\n",
       " (236, 186): 3,\n",
       " (186, 161): 2,\n",
       " (161, 236): 2,\n",
       " (165, 234): 1,\n",
       " (48, 32): 1,\n",
       " (156, 161): 1,\n",
       " (161, 234): 1,\n",
       " (176, 129): 1,\n",
       " (152, 149): 1,\n",
       " (149, 32): 1,\n",
       " (32, 76): 2,\n",
       " (76, 67): 1,\n",
       " (67, 75): 1,\n",
       " (75, 32): 1,\n",
       " (237, 157): 1,\n",
       " (157, 145): 1,\n",
       " (145, 234): 1,\n",
       " (129, 157): 1,\n",
       " (157, 235): 1,\n",
       " (130, 184): 1,\n",
       " (236, 135): 2,\n",
       " (135, 188): 2,\n",
       " (169, 148): 2,\n",
       " (147, 234): 1,\n",
       " (236, 147): 1,\n",
       " (147, 176): 1,\n",
       " (139, 157): 2,\n",
       " (186, 144): 1,\n",
       " (178, 149): 1,\n",
       " (236, 178): 2,\n",
       " (178, 180): 1,\n",
       " (235, 145): 1,\n",
       " (145, 152): 1,\n",
       " (235, 137): 1,\n",
       " (137, 152): 1,\n",
       " (150, 165): 3,\n",
       " (164, 128): 1,\n",
       " (50, 50): 1,\n",
       " (181, 172): 1,\n",
       " (184, 237): 1,\n",
       " (178, 168): 1,\n",
       " (164, 44): 1,\n",
       " (160, 237): 1,\n",
       " (150, 137): 1,\n",
       " (137, 237): 1,\n",
       " (48, 51): 1,\n",
       " (51, 93): 1,\n",
       " (93, 91): 2,\n",
       " (48, 52): 1,\n",
       " (52, 93): 1,\n",
       " (10, 234): 1,\n",
       " (235, 136): 1,\n",
       " (136, 136): 1,\n",
       " (149, 158): 2,\n",
       " (158, 236): 2,\n",
       " (165, 153): 1,\n",
       " (153, 237): 1,\n",
       " (131, 235): 2,\n",
       " (139, 136): 1,\n",
       " (141, 44): 1,\n",
       " (169, 32): 1,\n",
       " (236, 176): 1,\n",
       " (176, 190): 1,\n",
       " (190, 234): 1,\n",
       " (158, 173): 1,\n",
       " (160, 164): 1,\n",
       " (167, 132): 1,\n",
       " (159, 191): 1,\n",
       " (191, 32): 1,\n",
       " (48, 53): 1,\n",
       " (53, 93): 1,\n",
       " (48, 54): 1,\n",
       " (54, 93): 1,\n",
       " (93, 32): 1,\n",
       " (133, 237): 1,\n",
       " (151, 132): 1,\n",
       " (178, 173): 1,\n",
       " (130, 156): 1,\n",
       " (129, 188): 2,\n",
       " (185, 156): 2,\n",
       " (168, 234): 1,\n",
       " (176, 236): 1,\n",
       " (165, 235): 1,\n",
       " (160, 165): 1,\n",
       " (132, 152): 1,\n",
       " (140, 144): 2,\n",
       " (128, 44): 1,\n",
       " (76, 80): 1,\n",
       " (80, 76): 1,\n",
       " (76, 32): 1,\n",
       " (137, 235): 1,\n",
       " (179, 128): 1,\n",
       " (236, 161): 1,\n",
       " (161, 177): 1,\n",
       " (152, 181): 1,\n",
       " (236, 133): 2,\n",
       " (133, 152): 2,\n",
       " (176, 148): 2,\n",
       " (148, 234): 2,\n",
       " (234, 190): 2,\n",
       " (190, 188): 2,\n",
       " (153, 152): 1,\n",
       " (167, 164): 1,\n",
       " (235, 191): 1,\n",
       " (191, 144): 1,\n",
       " (137, 44): 1,\n",
       " (184, 46): 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find byte (or int here) pair statistics\n",
    "def byte_pair(tokens):\n",
    "    pairs = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        pairs[pair] = pairs.get(pair, 0) + 1\n",
    "    return pairs\n",
    "\n",
    "pairs = byte_pair(tokens)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(176, (32, 236)),\n",
       " (141, (236, 157)),\n",
       " (112, (32, 235)),\n",
       " (66, (156, 32)),\n",
       " (60, (157, 180)),\n",
       " (60, (32, 237)),\n",
       " (52, (237, 149)),\n",
       " (46, (132, 32)),\n",
       " (40, (236, 160)),\n",
       " (39, (235, 147))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by the most frequent byte pair occurances\n",
    "sorted(((v, k) for k,v in pairs.items()), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (32, 236) into a new token 256\n",
      "merging (236, 157) into a new token 257\n",
      "merging (32, 235) into a new token 258\n",
      "merging (32, 237) into a new token 259\n",
      "merging (257, 180) into a new token 260\n",
      "merging (237, 149) into a new token 261\n",
      "merging (32, 234) into a new token 262\n",
      "merging (235, 138) into a new token 263\n",
      "merging (235, 161) into a new token 264\n",
      "merging (156, 256) into a new token 265\n",
      "merging (257, 132) into a new token 266\n",
      "---------------------------\n",
      "merging (32, 236) into a new token 256\n",
      "merging (236, 157) into a new token 257\n",
      "merging (32, 235) into a new token 258\n",
      "merging (156, 32) into a new token 259\n",
      "merging (157, 180) into a new token 260\n",
      "merging (32, 237) into a new token 261\n",
      "merging (237, 149) into a new token 262\n",
      "merging (132, 32) into a new token 263\n",
      "merging (236, 160) into a new token 264\n",
      "merging (235, 147) into a new token 265\n",
      "merging (235, 161) into a new token 266\n"
     ]
    }
   ],
   "source": [
    "# merge byte pairs into new token\n",
    "def merge_tokens(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurrences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids): # iterate through all tokens\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "def merge_byte_pairs(ids, vocab_size):\n",
    "    # Each merge iteration will increase total voacb size by 1\n",
    "    vocab_idx = 256\n",
    "    merge_iter = vocab_size - vocab_idx\n",
    "    merges = {}  # (int, int) -> int\n",
    "    for _ in range(merge_iter):\n",
    "        stats = byte_pair(ids)\n",
    "        pair = max(stats, key=stats.get)\n",
    "        print(f\"merging {pair} into a new token {vocab_idx}\")\n",
    "        ids = merge_tokens(ids, pair, vocab_idx)\n",
    "        merges[pair] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "        \n",
    "    return ids, merges\n",
    "\n",
    "# This method prevents calling byte_pair every iterations\n",
    "def merge_byte_pairs_fast(ids, vocab_size):\n",
    "    # Each merge iteration will increase total voacb size by 1\n",
    "    vocab_idx = 256\n",
    "    merge_iter = vocab_size - vocab_idx\n",
    "    merges = {}  # (int, int) -> int\n",
    "    stats = byte_pair(ids)\n",
    "    pairs = sorted(stats.items(), key=lambda x: x[1], reverse=True)[:merge_iter]\n",
    "\n",
    "    for pair, _ in pairs:\n",
    "        print(f\"merging {pair} into a new token {vocab_idx}\")\n",
    "        ids = merge_tokens(ids, pair, vocab_idx)\n",
    "        merges[pair] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "            \n",
    "    return ids, merges\n",
    "\n",
    "# Final vocab size is 267\n",
    "vocab_size = 267\n",
    "\n",
    "# Call the new function to perform the merging\n",
    "ids, merges = merge_byte_pairs(tokens, vocab_size)\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Call the fast version of merging by calling pair stats only once\n",
    "fids, fmerges = merge_byte_pairs_fast(tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 4112\n",
      "BPE encoded tokens length: 3389\n",
      "compression ratio: 1.21X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"BPE encoded tokens length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 4112\n",
      "Fast BPE encoded tokens length: 3492\n",
      "Fast compression ratio: 1.18X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"Fast BPE encoded tokens length:\", len(fids))\n",
    "print(f\"Fast compression ratio: {len(tokens) / len(fids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: tokenization step is independent to the pretraining LLM model\n",
    "# raw text (unicode code point) ---- (tokenizer) ---> token sequence -> LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(262, b' \\xea'),\n",
       " (263, b'\\xeb\\x8a'),\n",
       " (264, b'\\xeb\\xa1'),\n",
       " (265, b'\\x9c \\xec'),\n",
       " (266, b'\\xec\\x9d\\x84')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding: given token sequence, convert into raw text\n",
    "\n",
    "vocab = {i: bytes([i]) for i in range(256)} # original bytes\n",
    "# merged tokens\n",
    "for (i1, i2), tokenidx in merges.items(): \n",
    "    vocab[tokenidx] = vocab[i1] + vocab[i2]\n",
    "list(vocab.items())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n페이커는 게임 내�'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(encodedtokens):\n",
    "    tokens = b\"\".join(vocab[i] for i in encodedtokens) # 1) token -> unicode byte\n",
    "    # default errors is \"strict\", replace prints invalid text instead without raising the error\n",
    "    # not all bytes are utf-8 decodable because of utf encoding rules\n",
    "    return tokens.decode(\"utf-8\", errors=\"replace\") # 2) unicode byte -> human readable text\n",
    "\n",
    "# � is not decodable token\n",
    "decode(ids[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[237, 142, 152, 260, 236, 187, 164, 262, 184, 176, 236, 138, 181, 236, 136, 173, 235, 176, 176]\n"
     ]
    }
   ],
   "source": [
    "# encoding: given raw text, convert into token sequences\n",
    "\n",
    "def encode(text, merges):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "  # tokens less than length 2 cannot be merged\n",
    "  while len(tokens) >= 2: \n",
    "    # find byte pairs to be merged dict((int, int): int)\n",
    "    pairs = byte_pair(tokens)\n",
    "    # Find the pair with the minimum value (=token) in the merges dictionary\n",
    "    # Recall merges = ((token1, token2): newtoken above value 255)\n",
    "    # Because early merges should be conducted first\n",
    "    pair = min(pairs, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge_tokens(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"페이커 기습숭배\", merges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encode() missing 1 required positional argument: 'merges'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# encode and decode are inverse process\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m decode(\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m자장면 짬뽕 탕수육 깐풍기\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: encode() missing 1 required positional argument: 'merges'"
     ]
    }
   ],
   "source": [
    "# encode and decode are inverse process\n",
    "decode(encode(\"자장면 짬뽕 탕수육 깐풍기\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex patterns used in GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"'re\", ' learning', ' NLP', ',', ' right', '?', ' I', \"'ve\", ' got', ' 10', ' tips', ' for', ' you', '!', '   ']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "# from gpt-2/src/encoder.py Encoder.pat\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    " \n",
    "print(re.findall(gpt2pat, \"You're learning NLP, right? I've got 10 tips for you!   \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **1. Matches Contractions (`'s|'t|'re|'ve|'m|'ll|'d`)**\n",
    "`'re` → Matches `\"You're\"`.\n",
    "`'ve` → Matches `\"I've\"`.\n",
    "\n",
    "###### **2. Matches Words (` ?\\p{L}+`)**\n",
    "`\"You\"`, `\"learning\"`, `\"NLP\"`, `\"right\"`, `\"I\"`, `\"got\"`, `\"tips\"`, `\"for\"`, `\"you\"`.\n",
    "\n",
    "######  **3. Matches Numbers (` ?\\p{N}+`)**\n",
    "`\"10\"`.\n",
    "\n",
    "###### **4. Matches Punctuation (` ?[^\\s\\p{L}\\p{N}]+`)**\n",
    "`\",\"`, `\"?\"`, `\"!\"`.\n",
    "\n",
    "###### **5. Matches Trailing Whitespace (`\\s+(?!\\S)`)**\n",
    "`\"   \"` (at the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# tiktoken library fast BPE tokeniser for openai models\n",
    "%pip install -q tiktoken\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 770, 318, 257, 2829, 1332, 284, 8996, 11241, 1634, 10064, 1022, 1180, 402, 11571, 3858]\n",
      "[262, 1115, 374, 264, 4382, 1296, 311, 9616, 4037, 2065, 15174, 1990, 2204, 480, 2898, 4595]\n",
      "[271, 1328, 382, 261, 4705, 1746, 316, 12221, 6602, 2860, 15142, 2870, 2647, 174803, 6009]\n"
     ]
    }
   ],
   "source": [
    "# Pattens are in https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    This is a simple test to compare tokenization strategies between different GPT types\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    This is a simple test to compare tokenization strategies between different GPT types\"))\n",
    "\n",
    "# GPT-4o (merges spaces, efficient token size)\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "print(enc.encode(\"    This is a simple test to compare tokenization strategies between different GPT types\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-19 14:53:44--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [application/octet-stream]\n",
      "Saving to: ‘vocab.bpe’\n",
      "\n",
      "vocab.bpe           100%[===================>] 445.62K   511KB/s    in 0.9s    \n",
      "\n",
      "2024-12-19 14:53:45 (511 KB/s) - ‘vocab.bpe’ saved [456318/456318]\n",
      "\n",
      "--2024-12-19 14:53:45--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: ‘encoder.json’\n",
      "\n",
      "encoder.json        100%[===================>]   1018K   744KB/s    in 1.4s    \n",
      "\n",
      "2024-12-19 14:53:48 (744 KB/s) - ‘encoder.json’ saved [1042301/1042301]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from gpt2 get_encoder func https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "# vocab.bpe: merges ((token1, token2) -> merged token id)\n",
    "# encoder.json: vocab (token integer -> byte)\n",
    "\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) \n",
    "\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 5 merges: [('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]\n",
      "Final 5 vocabs: [('om', 'inated'), ('Ġreg', 'ress'), ('ĠColl', 'ider'), ('Ġinform', 'ants'), ('Ġg', 'azed')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final 5 merges: {[(k, v) for k, v in encoder.items()][-5:]}\")\n",
    "print(f\"Final 5 vocabs: {bpe_merges[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of encoder: 50257\n",
      "lenght of vocab: 50000\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of encoder: {len(encoder)}\") # 256 raw byte tokens + 50000 merges + 1 special token\n",
    "print(f\"lenght of vocab: {len(bpe_merges)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token\n",
    "# token used to signal end of document\n",
    "encoder[\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: gpt-4o have 2 special tokens\n",
    "# can be found at https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L100C5-L100C62\n",
    "# special_tokens = {ENDOFTEXT: 199999, ENDOFPROMPT: 200018}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.merges = {}  # (int, int) -> int\n",
    "        self.utf_size = 256 # utf encoding size (before merging)\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        merge_iter = vocab_size - self.utf_size\n",
    "        for i in range(merge_iter):\n",
    "            stats = byte_pair(tokens)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            if verbose:\n",
    "                print(f\"merging {pair} into a new token {self.utf_size + i}\")\n",
    "            tokens = merge_tokens(tokens, pair, self.utf_size + i)\n",
    "            self.merges[pair] = self.utf_size + i\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "        while len(tokens) >= 2: \n",
    "            pairs = byte_pair(tokens)\n",
    "            pair = min(pairs, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = merge_tokens(tokens, pair, idx)\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        vocab = {i: bytes([i]) for i in range(256)} \n",
    "        for (i1, i2), tokenidx in self.merges.items(): \n",
    "            vocab[tokenidx] = vocab[i1] + vocab[i2]\n",
    "        \n",
    "        tokens = b\"\".join(vocab[i] for i in ids)  \n",
    "        return tokens.decode(\"utf-8\", errors=\"replace\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "basictokenizer = BasicTokenizer()\n",
    "\n",
    "with open(\"taylor.txt\", \"r\") as file:\n",
    "    txt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "basictokenizer.train(txt, 285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 97, 107, 101, 114, 32, 105, 115, 32, 116, 104, 101, 32, 98, 101, 115, 116, 32, 236, 136, 173, 235, 176, 176, 237, 149, 160, 32, 236, 139, 156, 234, 176, 132, 236, 157, 180, 236, 151, 144, 236, 154, 148]\n",
      "Faker is the best 숭배할 시간이에요\n"
     ]
    }
   ],
   "source": [
    "enc = basictokenizer.encode(\"Faker is the best 숭배할 시간이에요\")\n",
    "print(enc)\n",
    "\n",
    "dec = basictokenizer.decode(enc)\n",
    "print(dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencepiece is used in Llama and Mistral\n",
    "# they run bpe on unicode code points directly\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"In 2024, the world saw dramatic changes—politics, tech, and culture shifted. Social media platforms like X (formerly Twitter) & Meta thrived, while others faltered. Passwords like Pa$$w0rd! became outdated; biometrics took over. Emojis 🎉 & hashtags #Change2024 dominated conversations. At the heart of innovation, AI models (e.g., ChatGPT) continued their evolution. Companies discussed ethics: Should AIs say “yes” to every 🛠️? Or “no”? Meanwhile, everyday folks exclaimed, \\\"OMG! What a time to be alive! 😅.\\\" Data (1TB, 2TB) flowed rapidly, and files like report_v2_final.pdf defined workflows. In a fast-paced era, the question was simple: Where do we go next? 🌍\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: toy.txt\n",
      "  input_format: text\n",
      "  model_prefix: tok400\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 200000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 8\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: toy.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=665\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=66\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 91\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=20 all=403 active=337 piece=▁s\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=40 all=430 active=364 piece=TB\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=60 all=461 active=395 piece=and\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok400.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok400.vocab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# due to long history, sentencepiece has a lot of configurations\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization (popular config before LLM era, prefer not to touch in LLM era)\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment (important configs)\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules (simiilar to regex rules in tiktoken)\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist \n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2, # end of sentence\n",
    "  pad_id=-1, # -1, thus not use pad id\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['ed', 259],\n",
       " ['er', 260],\n",
       " ['▁t', 261],\n",
       " ['at', 262],\n",
       " ['an', 263],\n",
       " ['on', 264],\n",
       " ['or', 265],\n",
       " ['es', 266],\n",
       " ['he', 267],\n",
       " ['in', 268],\n",
       " ['li', 269],\n",
       " ['▁d', 270],\n",
       " ['▁e', 271],\n",
       " ['▁f', 272],\n",
       " ['▁w', 273],\n",
       " ['as', 274],\n",
       " ['hi', 275],\n",
       " ['▁c', 276],\n",
       " ['▁o', 277],\n",
       " ['▁s', 278],\n",
       " ['ion', 279],\n",
       " ['ver', 280],\n",
       " ['▁the', 281],\n",
       " ['al', 282],\n",
       " ['cs', 283],\n",
       " ['im', 284],\n",
       " ['ke', 285],\n",
       " ['le', 286],\n",
       " ['om', 287],\n",
       " ['▁(', 288],\n",
       " ['▁A', 289],\n",
       " ['▁a', 290],\n",
       " ['▁b', 291],\n",
       " ['▁li', 292],\n",
       " ['▁to', 293],\n",
       " ['▁like', 294],\n",
       " ['Ch', 295],\n",
       " ['In', 296],\n",
       " ['Me', 297],\n",
       " ['TB', 298],\n",
       " ['Wh', 299],\n",
       " ['am', 300],\n",
       " ['ay', 301],\n",
       " ['ec', 302],\n",
       " ['is', 303],\n",
       " ['ld', 304],\n",
       " ['lo', 305],\n",
       " ['ly', 306],\n",
       " ['mo', 307],\n",
       " ['no', 308],\n",
       " ['ol', 309],\n",
       " ['re', 310],\n",
       " ['ri', 311],\n",
       " ['ta', 312],\n",
       " ['ut', 313],\n",
       " ['▁&', 314],\n",
       " ['▁P', 315],\n",
       " ['▁S', 316],\n",
       " ['▁“', 317],\n",
       " ['and', 318],\n",
       " ['ang', 319],\n",
       " ['fin', 320],\n",
       " ['for', 321],\n",
       " ['low', 322],\n",
       " ['ter', 323],\n",
       " ['▁AI', 324],\n",
       " ['▁In', 325],\n",
       " ['▁Me', 326],\n",
       " ['▁Wh', 327],\n",
       " ['ated', 328],\n",
       " ['form', 329],\n",
       " ['hile', 330],\n",
       " ['very', 331],\n",
       " ['▁and', 332],\n",
       " ['▁con', 333],\n",
       " ['▁', 334],\n",
       " ['e', 335],\n",
       " ['a', 336],\n",
       " ['t', 337],\n",
       " ['o', 338],\n",
       " ['i', 339],\n",
       " ['s', 340],\n",
       " ['d', 341],\n",
       " ['r', 342],\n",
       " ['l', 343],\n",
       " ['n', 344],\n",
       " ['h', 345],\n",
       " ['c', 346],\n",
       " ['f', 347],\n",
       " ['m', 348],\n",
       " ['w', 349],\n",
       " [',', 350],\n",
       " ['.', 351],\n",
       " ['v', 352],\n",
       " ['p', 353],\n",
       " ['u', 354],\n",
       " ['y', 355],\n",
       " ['2', 356],\n",
       " ['k', 357],\n",
       " ['g', 358],\n",
       " ['I', 359],\n",
       " ['T', 360],\n",
       " ['!', 361],\n",
       " ['(', 362],\n",
       " [')', 363],\n",
       " ['0', 364],\n",
       " ['?', 365],\n",
       " ['A', 366],\n",
       " ['C', 367],\n",
       " ['M', 368],\n",
       " ['P', 369],\n",
       " ['b', 370],\n",
       " ['\"', 371],\n",
       " ['$', 372],\n",
       " ['&', 373],\n",
       " ['4', 374],\n",
       " [':', 375],\n",
       " ['B', 376],\n",
       " ['G', 377],\n",
       " ['O', 378],\n",
       " ['S', 379],\n",
       " ['W', 380],\n",
       " ['_', 381],\n",
       " ['x', 382],\n",
       " ['“', 383],\n",
       " ['”', 384],\n",
       " ['#', 385],\n",
       " ['-', 386],\n",
       " ['1', 387],\n",
       " [';', 388],\n",
       " ['D', 389],\n",
       " ['E', 390],\n",
       " ['X', 391],\n",
       " ['j', 392],\n",
       " ['q', 393],\n",
       " ['—', 394],\n",
       " ['️', 395],\n",
       " ['🌍', 396],\n",
       " ['🎉', 397],\n",
       " ['😅', 398],\n",
       " ['🛠', 399]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab # order: special token - byte tokens - merged tokens - raw codepoints tokens (in toy.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indivisual codepoint tokens should have low frequency (= rare)\n",
    "# character_coverage configs will let them not to be added to vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[334, 267, 343, 305, 334, 240, 145, 155, 239, 160, 183, 239, 190, 167, 334, 237, 187, 179, 239, 141, 184, 239, 139, 176, 238, 179, 179, 334, 239, 142, 159, 237, 179, 135, 239, 161, 136, 238, 142, 139, 238, 142, 167]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"hello 페이커 기습숭배 시간입니다\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'he', 'l', 'lo', '▁', '<0xED>', '<0x8E>', '<0x98>', '<0xEC>', '<0x9D>', '<0xB4>', '<0xEC>', '<0xBB>', '<0xA4>', '▁', '<0xEA>', '<0xB8>', '<0xB0>', '<0xEC>', '<0x8A>', '<0xB5>', '<0xEC>', '<0x88>', '<0xAD>', '<0xEB>', '<0xB0>', '<0xB0>', '▁', '<0xEC>', '<0x8B>', '<0x9C>', '<0xEA>', '<0xB0>', '<0x84>', '<0xEC>', '<0x9E>', '<0x85>', '<0xEB>', '<0x8B>', '<0x88>', '<0xEB>', '<0x8B>', '<0xA4>']\n"
     ]
    }
   ],
   "source": [
    "# Since korean characters are not trained (not included in toy.txt)\n",
    "# As byte_fallback=True, it encodes to utf-8 <0x..> tokens\n",
    "# If byte_fallback=false, it maps to <unk> token 0\n",
    "\n",
    "# Space switches spaces in to '_' (including first space - config add_dummy_prefix=True)\n",
    "# Let world in \"hello world\" and \"world\" have same token (\"world\" becomes \" world\")\n",
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding appropriate vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing vocab_size will increase computations\n",
    "# Because the final layers of LLM modles are logits/probs outputing layer nn.Linear(embed_size, vocab_size)\n",
    "# Large vocab_size may under train Embedding vector of some vocabs in nn.Embedding(vocab_size, embed_size)\n",
    "# When adding new tokens from existing model, initialize new vocabs' embedding vecotrs randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of other modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The architecture stays the same!\n",
    "# e.g. image - VQGAN, video - sora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
