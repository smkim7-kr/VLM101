{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eecaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install jax jaxlib flax optax tensorflow-datasets matplotlib torch tensorflow-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7457ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Tuple\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "\n",
    "# Helper Functions (modified for JAX)\n",
    "def to_one_hot(labels: jnp.ndarray, num_classes: int) -> jnp.ndarray:\n",
    "    return jax.nn.one_hot(labels, num_classes)\n",
    "\n",
    "# CIFAR-10 Dataset\n",
    "def load_cifar10(batch_size: int, split: str = 'train'):\n",
    "    ds = tfds.load('cifar10', split=split)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tfds.data.AUTOTUNE)\n",
    "    ds = tfds.as_numpy(ds)\n",
    "    return ds\n",
    "\n",
    "def prepare_batch(batch):\n",
    "  images = batch[\"image\"].astype(np.float32) / 255.\n",
    "  return jnp.asarray(images)\n",
    "\n",
    "# --- MODEL IMPLEMENTATION (Flax Modules) ---\n",
    "class Encoder(nn.Module):\n",
    "    latent_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(4, 4), strides=(2, 2))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=64, kernel_size=(4, 4), strides=(2, 2))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=128, kernel_size=(4, 4), strides=(2, 2))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=self.latent_dim, kernel_size=(2, 2), strides=(1, 1))(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    latent_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.ConvTranspose(features=128, kernel_size=(2, 2), strides=(1, 1))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.ConvTranspose(features=64, kernel_size=(4, 4), strides=(2, 2))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.ConvTranspose(features=32, kernel_size=(4, 4), strides=(2, 2))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.ConvTranspose(features=3, kernel_size=(4, 4), strides=(2, 2))(x)\n",
    "        x = jax.nn.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    num_embeddings: int\n",
    "    embedding_dim: int\n",
    "    commitment_cost: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: jnp.ndarray):\n",
    "        # Flatten input to (B, H*W, latent_dim)\n",
    "        flat_inputs = jnp.reshape(inputs, (inputs.shape[0], -1, self.embedding_dim))\n",
    "        # Initialize embedding vectors at the first run.\n",
    "        embeddings = self.param('embeddings', jax.random.normal, (self.num_embeddings, self.embedding_dim))\n",
    "\n",
    "        # Compute distances of flat_inputs to embeddings\n",
    "        distances = jnp.sum(flat_inputs**2, axis=2, keepdims=True) \\\n",
    "                      - 2 * jnp.matmul(flat_inputs, embeddings.T) \\\n",
    "                      + jnp.sum(embeddings**2, axis=1)\n",
    "        \n",
    "        # Get the index of nearest embedding to each flattened input vector\n",
    "        encoding_indices = jnp.argmin(distances, axis=2)\n",
    "        \n",
    "        # One-hot encode the indices\n",
    "        one_hot_encodings = to_one_hot(encoding_indices, self.num_embeddings)\n",
    "\n",
    "        # Quantize the encoded output from encoder using codebook. \n",
    "        quantized_outputs = jnp.matmul(one_hot_encodings, embeddings)\n",
    "        \n",
    "        # Reshape quantized output back to original encoded output shape.\n",
    "        quantized_outputs = jnp.reshape(quantized_outputs, inputs.shape)\n",
    "        \n",
    "        # Commitments\n",
    "        e_latent_loss = jnp.mean((jax.lax.stop_gradient(quantized_outputs) - inputs) ** 2)\n",
    "\n",
    "        # Commitment cost.\n",
    "        loss = self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Using straight-through estimator for better gradient propagation.\n",
    "        quantized_outputs = inputs + jax.lax.stop_gradient(quantized_outputs - inputs)\n",
    "        \n",
    "        return quantized_outputs, loss, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "  num_embeddings: int\n",
    "  embedding_dim: int\n",
    "  commitment_cost: float\n",
    "  \n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    encoder = Encoder(latent_dim=self.embedding_dim)\n",
    "    decoder = Decoder(latent_dim=self.embedding_dim)\n",
    "    vector_quantizer = VectorQuantizer(num_embeddings=self.num_embeddings, embedding_dim=self.embedding_dim, commitment_cost=self.commitment_cost)\n",
    "\n",
    "    z = encoder(x)\n",
    "    quantized_z, commitment_loss, encoding_indices  = vector_quantizer(z)\n",
    "    reconstructions = decoder(quantized_z)\n",
    "\n",
    "    return reconstructions, commitment_loss, encoding_indices\n",
    "\n",
    "# --- LOSS FUNCTIONS ---\n",
    "\n",
    "def reconstruction_loss(recon_x: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    return jnp.mean((recon_x - x) ** 2)\n",
    "\n",
    "\n",
    "def vq_vae_loss(\n",
    "    vqvae_output: Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray],\n",
    "    x: jnp.ndarray,\n",
    "    beta: float\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    reconstructions, commitment_loss, _ = vqvae_output\n",
    "    recon_loss = reconstruction_loss(reconstructions, x)\n",
    "    loss = recon_loss + beta * commitment_loss\n",
    "\n",
    "    metrics = {\n",
    "        'reconstruction_loss': recon_loss,\n",
    "        'commitment_loss': commitment_loss,\n",
    "        'loss': loss\n",
    "    }\n",
    "    return loss, metrics\n",
    "\n",
    "# --- TRAINING SETUP ---\n",
    "\n",
    "# State management\n",
    "class TrainState(train_state.TrainState):\n",
    "    vq_vae_params: flax.core.FrozenDict[str, Any]\n",
    "\n",
    "def create_train_state(rng, vqvae, learning_rate, input_shape):\n",
    "    params_rng, dropout_rng = jax.random.split(rng)\n",
    "    init_batch = jnp.zeros((1,) + input_shape)\n",
    "    variables = vqvae.init(params_rng, init_batch)\n",
    "    vq_vae_params = variables['params']\n",
    "\n",
    "    tx = optax.adam(learning_rate)\n",
    "\n",
    "    return TrainState.create(\n",
    "        apply_fn=vqvae.apply,\n",
    "        params = vq_vae_params,\n",
    "        tx=tx,\n",
    "        vq_vae_params=vq_vae_params,\n",
    "        dropout_rng=dropout_rng,\n",
    "        )\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state: TrainState, batch: jnp.ndarray, beta: float):\n",
    "\n",
    "  def loss_fn(params):\n",
    "    reconstructions, commitment_loss, _ = state.apply_fn({'params':params}, batch, rngs={'params': state.dropout_rng})\n",
    "    return vq_vae_loss((reconstructions, commitment_loss, _), batch, beta=beta)\n",
    "  \n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, metrics), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  return state, metrics\n",
    "\n",
    "# Evaluation Function\n",
    "@jax.jit\n",
    "def eval_step(state: TrainState, batch: jnp.ndarray, beta:float):\n",
    "    reconstructions, commitment_loss, _ = state.apply_fn({'params': state.params}, batch, rngs={'params': state.dropout_rng})\n",
    "    loss, metrics = vq_vae_loss((reconstructions, commitment_loss, _), batch, beta)\n",
    "    return metrics\n",
    "\n",
    "def train_model(\n",
    "    num_epochs: int = 10,\n",
    "    batch_size: int = 128,\n",
    "    learning_rate: float = 1e-3,\n",
    "    num_embeddings: int = 512,\n",
    "    embedding_dim: int = 64,\n",
    "    commitment_cost: float = 0.25,\n",
    "    beta: float = 1.0,\n",
    "    input_shape=(32, 32, 3),\n",
    "    visualization_epochs: int = 2\n",
    "):\n",
    "  # Data Setup\n",
    "    train_ds = load_cifar10(batch_size=batch_size, split='train')\n",
    "    test_ds = load_cifar10(batch_size=batch_size, split='test')\n",
    "\n",
    "    # Model and Optimizer Setup\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    key, params_rng = jax.random.split(key)\n",
    "    vqvae = VQVAE(num_embeddings=num_embeddings, embedding_dim=embedding_dim, commitment_cost=commitment_cost)\n",
    "\n",
    "    state = create_train_state(params_rng, vqvae, learning_rate, input_shape)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, batch in enumerate(train_ds):\n",
    "          images = prepare_batch(batch)\n",
    "          state, metrics = train_step(state, images, beta)\n",
    "\n",
    "        eval_metrics = {}\n",
    "        for batch_idx, batch in enumerate(test_ds):\n",
    "            images = prepare_batch(batch)\n",
    "            metrics = eval_step(state, images, beta)\n",
    "            for k, v in metrics.items():\n",
    "              eval_metrics[k] = eval_metrics.get(k, 0) + v / len(test_ds)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} -  Train Loss: {metrics[\"loss\"]:.4f}, Train Reconstruction Loss: {metrics[\"reconstruction_loss\"]:.4f}, Train Commitment Loss: {metrics[\"commitment_loss\"]:.4f}, Evaluation Loss: {eval_metrics[\"loss\"]:.4f}, Evaluation Reconstruction Loss: {eval_metrics[\"reconstruction_loss\"]:.4f}, Evaluation Commitment Loss: {eval_metrics[\"commitment_loss\"]:.4f}')\n",
    "        \n",
    "        # Visualize reconstructed images\n",
    "        if (epoch + 1) % visualization_epochs == 0:\n",
    "          for batch_idx, batch in enumerate(test_ds):\n",
    "            images = prepare_batch(batch)\n",
    "            reconstructions, _, _ = state.apply_fn({'params':state.params}, images, rngs={'params': state.dropout_rng})\n",
    "            \n",
    "            # Select a few images and their reconstructions\n",
    "            num_to_visualize = 4\n",
    "            images_to_plot = images[:num_to_visualize]\n",
    "            reconstructions_to_plot = reconstructions[:num_to_visualize]\n",
    "            \n",
    "            # Create figure\n",
    "            fig, axes = plt.subplots(2, num_to_visualize, figsize=(12, 6))\n",
    "            \n",
    "            for i in range(num_to_visualize):\n",
    "              axes[0, i].imshow(images_to_plot[i])\n",
    "              axes[0, i].set_title(\"Original\")\n",
    "              axes[0, i].axis('off')\n",
    "              axes[1, i].imshow(reconstructions_to_plot[i])\n",
    "              axes[1, i].set_title(\"Reconstructed\")\n",
    "              axes[1, i].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"reconstruction_epoch_{epoch+1}.png\")\n",
    "            plt.close(fig)\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
